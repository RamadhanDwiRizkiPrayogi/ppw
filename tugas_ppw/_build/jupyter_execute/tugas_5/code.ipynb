{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-10-13T05:19:27.700653Z",
     "iopub.status.busy": "2025-10-13T05:19:27.700653Z",
     "iopub.status.idle": "2025-10-13T05:19:35.252291Z",
     "shell.execute_reply": "2025-10-13T05:19:35.252291Z"
    },
    "id": "33OZHzkLJrts",
    "outputId": "37adac86-13d8-41ea-f4a7-9c0e69fe5944"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import string\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from gensim import corpora, models\n",
    "from gensim.models import CoherenceModel\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scraping berita kompas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T05:19:35.252291Z",
     "iopub.status.busy": "2025-10-13T05:19:35.252291Z",
     "iopub.status.idle": "2025-10-13T05:19:36.346702Z",
     "shell.execute_reply": "2025-10-13T05:19:36.346702Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping berhasil. Teks mentah siap diproses.\n"
     ]
    }
   ],
   "source": [
    "web = requests.get(\"https://surabaya.kompas.com/read/2025/10/13/073714878/keharuan-keluarga-saat-identitas-2-jasad-santri-ponpes-al-khoziny\")\n",
    "soup = BeautifulSoup(web.content, 'html.parser')\n",
    "for a in soup(['script', 'style']):\n",
    "    a.decompose()\n",
    "text = ' '.join(soup.stripped_strings)\n",
    "print(\"Scraping berhasil. Teks mentah siap diproses.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## inisisalisasi preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T05:19:36.424839Z",
     "iopub.status.busy": "2025-10-13T05:19:36.424839Z",
     "iopub.status.idle": "2025-10-13T05:19:36.640086Z",
     "shell.execute_reply": "2025-10-13T05:19:36.640086Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmer dan Stopwords siap digunakan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Membuat stemmer dari Sastrawi\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "\n",
    "# Mengunduh dan menyiapkan daftar stopwords Bahasa Indonesia\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('indonesian'))\n",
    "\n",
    "print(\"Stemmer dan Stopwords siap digunakan.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocessing per kalimat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-10-13T05:19:36.640086Z",
     "iopub.status.busy": "2025-10-13T05:19:36.640086Z",
     "iopub.status.idle": "2025-10-13T05:20:42.494784Z",
     "shell.execute_reply": "2025-10-13T05:20:42.494784Z"
    },
    "id": "QsC5GUtcWBEL",
    "outputId": "04981d7b-7221-41f7-9913-abc7f95d1248"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teks berhasil dipecah menjadi 60 kalimat.\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     14\u001b[39m sent = sent.strip()\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m#Stemming\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m stemmed_sent = \u001b[43mstemmer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstem\u001b[49m\u001b[43m(\u001b[49m\u001b[43msent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m#Tokenizing & Stopword Removal\u001b[39;00m\n\u001b[32m     20\u001b[39m tokens = [word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m stemmed_sent.split() \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stop_words]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\Sastrawi\\Stemmer\\CachedStemmer.py:20\u001b[39m, in \u001b[36mCachedStemmer.stem\u001b[39m\u001b[34m(self, text)\u001b[39m\n\u001b[32m     18\u001b[39m     stems.append(\u001b[38;5;28mself\u001b[39m.cache.get(word))\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m     stem = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdelegatedStemmer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m     \u001b[38;5;28mself\u001b[39m.cache.set(word, stem)\n\u001b[32m     22\u001b[39m     stems.append(stem)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\Sastrawi\\Stemmer\\Stemmer.py:27\u001b[39m, in \u001b[36mStemmer.stem\u001b[39m\u001b[34m(self, text)\u001b[39m\n\u001b[32m     24\u001b[39m stems = []\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words:\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m     stems.append(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstem_word\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m.join(stems)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\Sastrawi\\Stemmer\\Stemmer.py:36\u001b[39m, in \u001b[36mStemmer.stem_word\u001b[39m\u001b[34m(self, word)\u001b[39m\n\u001b[32m     34\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stem_plural_word(word)\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstem_singular_word\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\Sastrawi\\Stemmer\\Stemmer.py:84\u001b[39m, in \u001b[36mStemmer.stem_singular_word\u001b[39m\u001b[34m(self, word)\u001b[39m\n\u001b[32m     82\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Stem a singular word to its common stem form.\"\"\"\u001b[39;00m\n\u001b[32m     83\u001b[39m context = Context(word, \u001b[38;5;28mself\u001b[39m.dictionary, \u001b[38;5;28mself\u001b[39m.visitor_provider)\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     86\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m context.result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\Sastrawi\\Stemmer\\Context\\Context.py:37\u001b[39m, in \u001b[36mContext.execute\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     34\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Execute stemming process; the result can be retrieved with result\"\"\"\u001b[39;00m\n\u001b[32m     36\u001b[39m \u001b[38;5;66;03m#step 1 - 5\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstart_stemming_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m#step 6\u001b[39;00m\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dictionary.contains(\u001b[38;5;28mself\u001b[39m.current_word):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\Sastrawi\\Stemmer\\Context\\Context.py:80\u001b[39m, in \u001b[36mContext.start_stemming_process\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     77\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m     79\u001b[39m \u001b[38;5;66;03m#step 4, 5\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mremove_prefixes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dictionary.contains(\u001b[38;5;28mself\u001b[39m.current_word):\n\u001b[32m     82\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\Sastrawi\\Stemmer\\Context\\Context.py:89\u001b[39m, in \u001b[36mContext.remove_prefixes\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     87\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mremove_prefixes\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m     88\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m3\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m89\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maccept_prefix_visitors\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprefix_pisitors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     90\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dictionary.contains(\u001b[38;5;28mself\u001b[39m.current_word):\n\u001b[32m     91\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\Sastrawi\\Stemmer\\Context\\Context.py:110\u001b[39m, in \u001b[36mContext.accept_prefix_visitors\u001b[39m\u001b[34m(self, visitors)\u001b[39m\n\u001b[32m    108\u001b[39m removalCount = \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.removals)\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m visitor \u001b[38;5;129;01min\u001b[39;00m visitors:\n\u001b[32m--> \u001b[39m\u001b[32m110\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maccept\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvisitor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    111\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dictionary.contains(\u001b[38;5;28mself\u001b[39m.current_word):\n\u001b[32m    112\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.current_word\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\Sastrawi\\Stemmer\\Context\\Context.py:96\u001b[39m, in \u001b[36mContext.accept\u001b[39m\u001b[34m(self, visitor)\u001b[39m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mremove_suffixes\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m     94\u001b[39m     \u001b[38;5;28mself\u001b[39m.accept_visitors(\u001b[38;5;28mself\u001b[39m.suffix_visitors)\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34maccept\u001b[39m(\u001b[38;5;28mself\u001b[39m, visitor):\n\u001b[32m     97\u001b[39m     visitor.visit(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m     99\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34maccept_visitors\u001b[39m(\u001b[38;5;28mself\u001b[39m, visitors):\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Memecah teks mentah menjadi daftar kalimat (sentences)\n",
    "sentences = re.split(r'[.!?]\\s*', text) # Memecah berdasarkan tanda titik, tanda tanya, atau tanda seru diikuti oleh spasi\n",
    "print(f\"Teks berhasil dipecah menjadi {len(sentences)} kalimat.\\n\") # Ditambah \\n\n",
    "\n",
    "#perulangan untuk preprocessing pada setiap kalimat \n",
    "processed_docs = []\n",
    "for sentence in sentences:\n",
    "    # Hanya proses kalimat yang tidak terlalu pendek\n",
    "    if len(sentence) > 20:\n",
    "        #Case Folding & Cleaning (angka, tanda baca)\n",
    "        sent = sentence.lower()\n",
    "        sent = re.sub(r'\\d+', '', sent)\n",
    "        sent = sent.translate(str.maketrans('', '', string.punctuation))\n",
    "        sent = sent.strip()\n",
    "\n",
    "        #Stemming\n",
    "        stemmed_sent = stemmer.stem(sent)\n",
    "\n",
    "        #Tokenizing & Stopword Removal\n",
    "        tokens = [word for word in stemmed_sent.split() if word not in stop_words]\n",
    "\n",
    "        # Tambahkan hasil (daftar token) jika tidak kosong\n",
    "        if tokens:\n",
    "            processed_docs.append(tokens)\n",
    "\n",
    "print(f\"Preprocessing selesai. Dihasilkan {len(processed_docs)} dokumen yang siap diproses.\")\n",
    "print(\"\\nContoh 3 dokumen pertama:\")\n",
    "for i, doc in enumerate(processed_docs[:3]):\n",
    "    print(f\"Dokumen {i}: {doc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-10-13T05:20:42.494784Z",
     "iopub.status.busy": "2025-10-13T05:20:42.494784Z",
     "iopub.status.idle": "2025-10-13T05:20:42.515523Z",
     "shell.execute_reply": "2025-10-13T05:20:42.515523Z"
    },
    "id": "2oriBqcCZ3zz",
    "outputId": "14dcc0ee-8b7c-4d27-9073-fa724d521ebf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Corpus berhasil dibuat.\n",
      "Contoh representasi BoW untuk dokumen pertama:\n",
      "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1)]\n"
     ]
    }
   ],
   "source": [
    "# Membuat kamus: memetakan setiap kata unik ke sebuah ID\n",
    "dictionary = corpora.Dictionary(processed_docs)\n",
    "\n",
    "# (Opsional) Filter kata-kata yang terlalu jarang atau terlalu sering muncul\n",
    "# no_below=2 : mengabaikan kata yang muncul di kurang dari 2 dokumen\n",
    "# no_above=0.7 : mengabaikan kata yang muncul di lebih dari 70% dokumen\n",
    "dictionary.filter_extremes(no_below=2, no_above=0.7)\n",
    "\n",
    "# Membuat corpus: mengubah setiap dokumen menjadi representasi BoW (ID kata, frekuensi)\n",
    "corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "print(\"\\nCorpus berhasil dibuat.\")\n",
    "print(\"Contoh representasi BoW untuk dokumen pertama:\")\n",
    "print(corpus[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T05:20:42.523005Z",
     "iopub.status.busy": "2025-10-13T05:20:42.515523Z",
     "iopub.status.idle": "2025-10-13T05:20:43.176323Z",
     "shell.execute_reply": "2025-10-13T05:20:43.176323Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memulai pelatihan model LDA untuk menemukan 5 topik...\n",
      "Model LDA berhasil dilatih!\n"
     ]
    }
   ],
   "source": [
    "# Tentukan jumlah topik yang ingin dicari\n",
    "num_topics = 5\n",
    "\n",
    "print(f\"Memulai pelatihan model LDA untuk menemukan {num_topics} topik...\")\n",
    "if not corpus or not dictionary:\n",
    "    print(\"Corpus atau Dictionary kosong. Tidak dapat melatih model LDA.\")\n",
    "else:\n",
    "    lda_model = models.LdaModel(\n",
    "        corpus=corpus,\n",
    "        id2word=dictionary,\n",
    "        num_topics=num_topics,\n",
    "        random_state=100,  # Agar hasil bisa direproduksi\n",
    "        passes=15          # Jumlah iterasi pelatihan\n",
    "    )\n",
    "    print(\"Model LDA berhasil dilatih!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T05:20:43.176323Z",
     "iopub.status.busy": "2025-10-13T05:20:43.176323Z",
     "iopub.status.idle": "2025-10-13T05:20:43.562063Z",
     "shell.execute_reply": "2025-10-13T05:20:43.562063Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topik-topik yang ditemukan oleh model LDA:\n",
      "--------------------------------------------------\n",
      "Topik: 0 \n",
      "Kata Kunci: 0.061*\"com\" + 0.037*\"kompas\" + 0.030*\"berita\" + 0.024*\"sekolah\" + 0.024*\"iklan\" + 0.024*\"tewas\" + 0.024*\"smp\" + 0.024*\"media\" + 0.022*\"baca\" + 0.019*\"grobogan\"\n",
      "\n",
      "Topik: 1 \n",
      "Kata Kunci: 0.043*\"nama\" + 0.043*\"jenazah\" + 0.023*\"hasil\" + 0.023*\"com\" + 0.023*\"daftar\" + 0.023*\"haikal\" + 0.023*\"foto\" + 0.023*\"makam\" + 0.023*\"lihat\" + 0.023*\"kantong\"\n",
      "\n",
      "Topik: 2 \n",
      "Kata Kunci: 0.064*\"jenazah\" + 0.052*\"kantong\" + 0.039*\"dna\" + 0.038*\"body\" + 0.033*\"dvi\" + 0.033*\"identifikasi\" + 0.032*\"bangkal\" + 0.032*\"part\" + 0.032*\"cocok\" + 0.027*\"nomor\"\n",
      "\n",
      "Topik: 3 \n",
      "Kata Kunci: 0.072*\"surabaya\" + 0.072*\"ponpes\" + 0.052*\"al\" + 0.050*\"korban\" + 0.049*\"wib\" + 0.049*\"khoziny\" + 0.038*\"identifikasi\" + 0.025*\"keluarga\" + 0.023*\"ambruk\" + 0.017*\"santri\"\n",
      "\n",
      "Topik: 4 \n",
      "Kata Kunci: 0.032*\"properti\" + 0.027*\"news\" + 0.022*\"didik\" + 0.022*\"umkm\" + 0.022*\"travel\" + 0.022*\"indonesia\" + 0.020*\"artikel\" + 0.019*\"kompas\" + 0.017*\"jawa\" + 0.017*\"sehat\"\n",
      "\n",
      "--------------------------------------------------\n",
      "Coherence Score (c_v): 0.4458\n",
      "(Skor Coherence yang baik biasanya mendekati 1.0)\n"
     ]
    }
   ],
   "source": [
    "if 'lda_model' in locals():\n",
    "    print(\"Topik-topik yang ditemukan oleh model LDA:\")\n",
    "    print(\"-\" * 50)\n",
    "    for idx, topic in lda_model.print_topics(-1):\n",
    "        print(f\"Topik: {idx} \\nKata Kunci: {topic}\\n\")\n",
    "\n",
    "    # Evaluasi dengan Coherence Score dengan c_v\n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=processed_docs, dictionary=dictionary, coherence='c_v')\n",
    "    coherence_lda = coherence_model_lda.get_coherence()\n",
    "    print(\"-\" * 50)\n",
    "    print(f'Coherence Score (c_v): {coherence_lda:.4f}')\n",
    "    print(\"(Skor Coherence yang baik biasanya mendekati 1.0)\")\n",
    "else:\n",
    "    print(\"Model LDA belum dilatih. Jalankan sel sebelumnya terlebih dahulu.\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}