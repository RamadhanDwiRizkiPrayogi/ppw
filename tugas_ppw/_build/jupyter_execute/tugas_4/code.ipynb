{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openpyxl in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.1.5)\n",
      "Requirement already satisfied: xlsxwriter in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.2.9)\n",
      "Requirement already satisfied: et-xmlfile in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openpyxl) (2.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "%pip install openpyxl xlsxwriter\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Konstanta\n",
    "BASE_URL = \"https://pta.trunojoyo.ac.id/c_search/byprod\"\n",
    "\n",
    "def get_max_page(prodi_id):\n",
    "    \"\"\"Mendapatkan jumlah halaman maksimal untuk prodi tertentu\"\"\"\n",
    "    try:\n",
    "        url = f\"{BASE_URL}/{prodi_id}/1\"\n",
    "        response = requests.get(url, timeout=10)\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        \n",
    "        pagination = soup.select('a[href*=\"byprod\"]')\n",
    "        if pagination:\n",
    "            last_page = 1\n",
    "            for link in pagination:\n",
    "                href = link.get('href', '')\n",
    "                if f'/byprod/{prodi_id}/' in href:\n",
    "                    page_match = re.search(rf'/byprod/{prodi_id}/(\\d+)', href)\n",
    "                    if page_match:\n",
    "                        page_num = int(page_match.group(1))\n",
    "                        last_page = max(last_page, page_num)\n",
    "            return last_page\n",
    "        return 1\n",
    "    except:\n",
    "        return 1\n",
    "\n",
    "def print_progress_bar(prodi_id, prodi_name, current_page, max_page, total_entries):\n",
    "    \"\"\"Menampilkan progress bar sederhana\"\"\"\n",
    "    progress = (current_page / max_page) * 100\n",
    "    filled = int(progress // 5)  # Setiap 5% = 1 blok\n",
    "    bar = '‚ñà' * filled + '‚ñë' * (20 - filled)\n",
    "    \n",
    "    print(f\"\\r[{prodi_id}] {prodi_name} - Page {current_page}/{max_page} [{bar}] {progress:.2f}%\", end='', flush=True)\n",
    "\n",
    "def pta():\n",
    "    \"\"\"Fungsi utama untuk scraping data PTA\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    data = {\n",
    "        \"id\": [], \"penulis\": [], \"judul\": [], \"abstrak_id\": [], \"abstrak_en\": [],\n",
    "        \"pembimbing_pertama\": [], \"pembimbing_kedua\": [], \"prodi\": []\n",
    "    }\n",
    "    \n",
    "    prodi_id = 7\n",
    "    prodi_name = \"Manajemen\"\n",
    "    \n",
    "    # Get max pages\n",
    "    max_page = get_max_page(prodi_id)\n",
    "    \n",
    "    for j in range(1, max_page + 1):\n",
    "        try:\n",
    "            url = f\"{BASE_URL}/{prodi_id}/{j}\"\n",
    "            r = requests.get(url, timeout=15)\n",
    "            soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "            jurnals = soup.select('li[data-cat=\"#luxury\"]')\n",
    "            \n",
    "            # Process setiap jurnal\n",
    "            for jurnal in jurnals:\n",
    "                try:\n",
    "                    link_keluar = jurnal.select_one('a.gray.button')['href']\n",
    "                    \n",
    "                    # Ambil ID dari link PTA\n",
    "                    id_match = re.search(r\"/detail/(\\d+)\", link_keluar)\n",
    "                    pta_id = id_match.group(1) if id_match else None\n",
    "                    \n",
    "                    response = requests.get(link_keluar, timeout=15)\n",
    "                    soup1 = BeautifulSoup(response.content, \"html.parser\")\n",
    "                    isi = soup1.select_one('div#content_journal')\n",
    "                    \n",
    "                    if not isi:\n",
    "                        continue\n",
    "                    \n",
    "                    # Extract data\n",
    "                    judul = isi.select_one('a.title').text.strip()\n",
    "                    \n",
    "                    # Handle fields dengan error handling\n",
    "                    try:\n",
    "                        penulis = isi.select_one('span:contains(\"Penulis\")').text.split(' : ')[1]\n",
    "                    except:\n",
    "                        penulis = \"N/A\"\n",
    "                    \n",
    "                    try:\n",
    "                        pembimbing_pertama = isi.select_one('span:contains(\"Dosen Pembimbing I\")').text.split(' : ')[1]\n",
    "                    except:\n",
    "                        pembimbing_pertama = \"N/A\"\n",
    "                    \n",
    "                    try:\n",
    "                        pembimbing_kedua = isi.select_one('span:contains(\"Dosen Pembimbing II\")').text.split(' : ')[1]\n",
    "                    except:\n",
    "                        pembimbing_kedua = \"N/A\"\n",
    "                    \n",
    "                    # Extract abstrak\n",
    "                    paragraf = isi.select('p[align=\"justify\"]')\n",
    "                    abstrak_id = paragraf[0].get_text(strip=True) if len(paragraf) > 0 else \"N/A\"\n",
    "                    abstrak_en = paragraf[1].get_text(strip=True) if len(paragraf) > 1 else \"N/A\"\n",
    "                    \n",
    "                    # Clean abstrak Indonesia\n",
    "                    if abstrak_id != \"N/A\" and abstrak_id.upper().startswith('ABSTRAK'):\n",
    "                        abstrak_id = abstrak_id[7:].strip()\n",
    "                    \n",
    "                    # Append data\n",
    "                    data[\"id\"].append(pta_id)\n",
    "                    data[\"penulis\"].append(penulis)\n",
    "                    data[\"judul\"].append(judul)\n",
    "                    data[\"abstrak_id\"].append(abstrak_id)\n",
    "                    data[\"abstrak_en\"].append(abstrak_en)\n",
    "                    data[\"pembimbing_pertama\"].append(pembimbing_pertama)\n",
    "                    data[\"pembimbing_kedua\"].append(pembimbing_kedua)\n",
    "                    data[\"prodi\"].append(prodi_name)\n",
    "                    \n",
    "                    time.sleep(0.1)\n",
    "                    \n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            # Update progress bar\n",
    "            print_progress_bar(prodi_id, prodi_name, j, max_page, len(data['id']))\n",
    "            \n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    # New line setelah progress selesai\n",
    "    print()\n",
    "    \n",
    "    # Simpan data dengan error handling\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Simpan ke CSV (selalu berhasil)\n",
    "    df.to_csv(\"pta_lengkap.csv\", index=False)\n",
    "    print(\"üíæ Data disimpan ke: pta_lengkap.csv\")\n",
    "    \n",
    "    # Coba simpan ke Excel\n",
    "    try:\n",
    "        df.to_excel(\"pta_lengkap.xlsx\", index=False, engine='openpyxl')\n",
    "        print(\"üíæ Data disimpan ke: pta_lengkap.xlsx\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Excel tidak dapat disimpan: {e}\")\n",
    "        print(\"üìù Data tetap tersimpan dalam format CSV\")\n",
    "    \n",
    "    # Hitung durasi\n",
    "    end_time = time.time()\n",
    "    elapsed = int(end_time - start_time)\n",
    "    jam, sisa = divmod(elapsed, 3600)\n",
    "    menit, detik = divmod(sisa, 60)\n",
    "    \n",
    "    # Summary\n",
    "    print(f\"\\nüìä Total data dikumpulkan: {len(df):,}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Jalankan scraping\n",
    "result_df = pta()\n",
    "\n",
    "# Ambil abstrak Indonesia untuk corpus\n",
    "corpus = result_df[result_df['abstrak_id'] != 'N/A']['abstrak_id'].tolist()\n",
    "\n",
    "# Tampilkan dalam bentuk DataFrame (semua data)\n",
    "import pandas as pd\n",
    "df_sample = pd.DataFrame({\n",
    "    'No': range(1, len(corpus) + 1),\n",
    "    'Abstrak': corpus\n",
    "})\n",
    "\n",
    "df_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "75acd8bf",
    "outputId": "d3f0bc6c-6af3-4d86-952e-b0e374355ff4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.32.5)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.13.4)\n",
      "Requirement already satisfied: lxml in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (6.0.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests) (2025.8.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from beautifulsoup4) (2.7)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from beautifulsoup4) (4.14.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install requests beautifulsoup4 lxml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "11UFB9KvOQyP"
   },
   "source": [
    "#pembersihan teks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hasil Pembersihan Teks:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstrak_asli</th>\n",
       "      <th>abstrak_bersih</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Satiyah, Pengaruh Faktor-faktor Pelatihan dan ...</td>\n",
       "      <td>satiyah pengaruh faktorfaktor pelatihan dan pe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tujuan penelitian ini adalah untuk mengetahui ...</td>\n",
       "      <td>tujuan penelitian ini adalah untuk mengetahui ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Aplikasi nyata pemanfaatan teknologi informasi...</td>\n",
       "      <td>aplikasi nyata pemanfaatan teknologi informasi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Penelitian ini menggunakan metode kuantitatif,...</td>\n",
       "      <td>penelitian ini menggunakan metode kuantitatif ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Aththaariq, Pengaruh Kompetensi Dosen Terhadap...</td>\n",
       "      <td>aththaariq pengaruh kompetensi dosen terhadap ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Haryono Arifin, Pengaruh Perilaku Konsumen Ter...</td>\n",
       "      <td>haryono arifin pengaruh perilaku konsumen terh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Dharma Abidin Syah,Kesimpulan: (1) Terdapat pe...</td>\n",
       "      <td>dharma abidin syahkesimpulan terdapat pengaruh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Tujuan penelitian ini adalah untuk mengidentif...</td>\n",
       "      <td>tujuan penelitian ini adalah untuk mengidentif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Hasil dari penelitian ini dari perhitungan Cre...</td>\n",
       "      <td>hasil dari penelitian ini dari perhitungan cre...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        abstrak_asli  \\\n",
       "0  Satiyah, Pengaruh Faktor-faktor Pelatihan dan ...   \n",
       "1  Tujuan penelitian ini adalah untuk mengetahui ...   \n",
       "2                                                      \n",
       "3  Aplikasi nyata pemanfaatan teknologi informasi...   \n",
       "4  Penelitian ini menggunakan metode kuantitatif,...   \n",
       "5  Aththaariq, Pengaruh Kompetensi Dosen Terhadap...   \n",
       "6  Haryono Arifin, Pengaruh Perilaku Konsumen Ter...   \n",
       "7  Dharma Abidin Syah,Kesimpulan: (1) Terdapat pe...   \n",
       "8  Tujuan penelitian ini adalah untuk mengidentif...   \n",
       "9  Hasil dari penelitian ini dari perhitungan Cre...   \n",
       "\n",
       "                                      abstrak_bersih  \n",
       "0  satiyah pengaruh faktorfaktor pelatihan dan pe...  \n",
       "1  tujuan penelitian ini adalah untuk mengetahui ...  \n",
       "2                                                     \n",
       "3  aplikasi nyata pemanfaatan teknologi informasi...  \n",
       "4  penelitian ini menggunakan metode kuantitatif ...  \n",
       "5  aththaariq pengaruh kompetensi dosen terhadap ...  \n",
       "6  haryono arifin pengaruh perilaku konsumen terh...  \n",
       "7  dharma abidin syahkesimpulan terdapat pengaruh...  \n",
       "8  tujuan penelitian ini adalah untuk mengidentif...  \n",
       "9  hasil dari penelitian ini dari perhitungan cre...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pembersihan teks\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def clean_text(text):\n",
    "    # Ubah ke huruf kecil\n",
    "    text = text.lower()\n",
    "    # Hapus angka\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # Hapus tanda baca\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Hapus spasi berlebih\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# Terapkan pembersihan teks\n",
    "cleaned_corpus = [clean_text(text) for text in corpus]\n",
    "\n",
    "# Tampilkan hasil pembersihan teks dalam format DataFrame\n",
    "comparison_df = pd.DataFrame({\n",
    "    'abstrak_asli': corpus[:10],\n",
    "    'abstrak_bersih': cleaned_corpus[:10]\n",
    "})\n",
    "\n",
    "print(\"\\nHasil Pembersihan Teks:\")\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k9j1pNkvOVKo"
   },
   "source": [
    "#tokenisasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yAJTtGY7OXCP",
    "outputId": "a22fe187-e3b5-4da4-a808-757a191de6e7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PTA (abstrak_id_tokens):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstrak_id_clean</th>\n",
       "      <th>abstrak_id_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>satiyah pengaruh faktorfaktor pelatihan dan pe...</td>\n",
       "      <td>[satiyah, pengaruh, faktorfaktor, pelatihan, d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tujuan penelitian ini adalah untuk mengetahui ...</td>\n",
       "      <td>[tujuan, penelitian, ini, adalah, untuk, menge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aplikasi nyata pemanfaatan teknologi informasi...</td>\n",
       "      <td>[aplikasi, nyata, pemanfaatan, teknologi, info...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>penelitian ini menggunakan metode kuantitatif ...</td>\n",
       "      <td>[penelitian, ini, menggunakan, metode, kuantit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>aththaariq pengaruh kompetensi dosen terhadap ...</td>\n",
       "      <td>[aththaariq, pengaruh, kompetensi, dosen, terh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>haryono arifin pengaruh perilaku konsumen terh...</td>\n",
       "      <td>[haryono, arifin, pengaruh, perilaku, konsumen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>dharma abidin syahkesimpulan terdapat pengaruh...</td>\n",
       "      <td>[dharma, abidin, syahkesimpulan, terdapat, pen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>tujuan penelitian ini adalah untuk mengidentif...</td>\n",
       "      <td>[tujuan, penelitian, ini, adalah, untuk, mengi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>hasil dari penelitian ini dari perhitungan cre...</td>\n",
       "      <td>[hasil, dari, penelitian, ini, dari, perhitung...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    abstrak_id_clean  \\\n",
       "0  satiyah pengaruh faktorfaktor pelatihan dan pe...   \n",
       "1  tujuan penelitian ini adalah untuk mengetahui ...   \n",
       "2                                                      \n",
       "3  aplikasi nyata pemanfaatan teknologi informasi...   \n",
       "4  penelitian ini menggunakan metode kuantitatif ...   \n",
       "5  aththaariq pengaruh kompetensi dosen terhadap ...   \n",
       "6  haryono arifin pengaruh perilaku konsumen terh...   \n",
       "7  dharma abidin syahkesimpulan terdapat pengaruh...   \n",
       "8  tujuan penelitian ini adalah untuk mengidentif...   \n",
       "9  hasil dari penelitian ini dari perhitungan cre...   \n",
       "\n",
       "                                   abstrak_id_tokens  \n",
       "0  [satiyah, pengaruh, faktorfaktor, pelatihan, d...  \n",
       "1  [tujuan, penelitian, ini, adalah, untuk, menge...  \n",
       "2                                                 []  \n",
       "3  [aplikasi, nyata, pemanfaatan, teknologi, info...  \n",
       "4  [penelitian, ini, menggunakan, metode, kuantit...  \n",
       "5  [aththaariq, pengaruh, kompetensi, dosen, terh...  \n",
       "6  [haryono, arifin, pengaruh, perilaku, konsumen...  \n",
       "7  [dharma, abidin, syahkesimpulan, terdapat, pen...  \n",
       "8  [tujuan, penelitian, ini, adalah, untuk, mengi...  \n",
       "9  [hasil, dari, penelitian, ini, dari, perhitung...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import pandas as pd\n",
    "\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# Tokenisasi untuk PTA\n",
    "# Buat DataFrame dari corpus yang sudah dibersihkan\n",
    "pta_df = pd.DataFrame({\n",
    "    'abstrak_id_clean': cleaned_corpus\n",
    "})\n",
    "\n",
    "pta_df[\"abstrak_id_tokens\"] = pta_df[\"abstrak_id_clean\"].apply(word_tokenize)\n",
    "\n",
    "print(\"\\nPTA (abstrak_id_tokens):\")\n",
    "pta_df[[\"abstrak_id_clean\", \"abstrak_id_tokens\"]].head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7MJMFTQVOarI"
   },
   "source": [
    "#Penghapusan Kata Umum (Stop Words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PKkRL87EOben",
    "outputId": "775733a7-511b-447e-c341-f1c5b46a2274"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PTA (abstrak_id_filtered):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstrak_id_tokens</th>\n",
       "      <th>abstrak_id_filtered</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[satiyah, pengaruh, faktorfaktor, pelatihan, d...</td>\n",
       "      <td>[satiyah, pengaruh, faktorfaktor, pelatihan, p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[tujuan, penelitian, ini, adalah, untuk, menge...</td>\n",
       "      <td>[tujuan, penelitian, persepsi, brand, associat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[aplikasi, nyata, pemanfaatan, teknologi, info...</td>\n",
       "      <td>[aplikasi, nyata, pemanfaatan, teknologi, info...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[penelitian, ini, menggunakan, metode, kuantit...</td>\n",
       "      <td>[penelitian, metode, kuantitatif, menekankan, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[aththaariq, pengaruh, kompetensi, dosen, terh...</td>\n",
       "      <td>[aththaariq, pengaruh, kompetensi, dosen, kine...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[haryono, arifin, pengaruh, perilaku, konsumen...</td>\n",
       "      <td>[haryono, arifin, pengaruh, perilaku, konsumen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[dharma, abidin, syahkesimpulan, terdapat, pen...</td>\n",
       "      <td>[dharma, abidin, syahkesimpulan, pengaruh, sig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[tujuan, penelitian, ini, adalah, untuk, mengi...</td>\n",
       "      <td>[tujuan, penelitian, mengidentifikasi, variabe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[hasil, dari, penelitian, ini, dari, perhitung...</td>\n",
       "      <td>[hasil, penelitian, perhitungan, credit, risk,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   abstrak_id_tokens  \\\n",
       "0  [satiyah, pengaruh, faktorfaktor, pelatihan, d...   \n",
       "1  [tujuan, penelitian, ini, adalah, untuk, menge...   \n",
       "2                                                 []   \n",
       "3  [aplikasi, nyata, pemanfaatan, teknologi, info...   \n",
       "4  [penelitian, ini, menggunakan, metode, kuantit...   \n",
       "5  [aththaariq, pengaruh, kompetensi, dosen, terh...   \n",
       "6  [haryono, arifin, pengaruh, perilaku, konsumen...   \n",
       "7  [dharma, abidin, syahkesimpulan, terdapat, pen...   \n",
       "8  [tujuan, penelitian, ini, adalah, untuk, mengi...   \n",
       "9  [hasil, dari, penelitian, ini, dari, perhitung...   \n",
       "\n",
       "                                 abstrak_id_filtered  \n",
       "0  [satiyah, pengaruh, faktorfaktor, pelatihan, p...  \n",
       "1  [tujuan, penelitian, persepsi, brand, associat...  \n",
       "2                                                 []  \n",
       "3  [aplikasi, nyata, pemanfaatan, teknologi, info...  \n",
       "4  [penelitian, metode, kuantitatif, menekankan, ...  \n",
       "5  [aththaariq, pengaruh, kompetensi, dosen, kine...  \n",
       "6  [haryono, arifin, pengaruh, perilaku, konsumen...  \n",
       "7  [dharma, abidin, syahkesimpulan, pengaruh, sig...  \n",
       "8  [tujuan, penelitian, mengidentifikasi, variabe...  \n",
       "9  [hasil, penelitian, perhitungan, credit, risk,...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "# Download stopwords untuk bahasa Indonesia\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Stopwords untuk bahasa Indonesia\n",
    "stop_words_id = set(stopwords.words('indonesian'))\n",
    "\n",
    "# Filter stopwords di PTA\n",
    "pta_df[\"abstrak_id_filtered\"] = pta_df[\"abstrak_id_tokens\"].apply(\n",
    "    lambda tokens: [word for word in tokens if word not in stop_words_id]\n",
    ")\n",
    "\n",
    "print(\"\\nPTA (abstrak_id_filtered):\")\n",
    "pta_df[[\"abstrak_id_tokens\", \"abstrak_id_filtered\"]].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sCT0BB3ZOdMY"
   },
   "source": [
    "#Stemming dan Lematisasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x1Q44O9SOfwx",
    "outputId": "8b5d33a6-0d03-4c67-a75f-76f72719f2e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Sastrawi in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m factory = StemmerFactory()\n\u001b[32m      7\u001b[39m indo_stemmer = factory.create_stemmer()\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m pta_df[\u001b[33m\"\u001b[39m\u001b[33mabstrak_id_stemmed\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[43mpta_df\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mabstrak_id_filtered\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mindo_stemmer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mword\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mPTA - Stemming & Lemmatization (abstrak_id):\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     14\u001b[39m pta_df[[\u001b[33m\"\u001b[39m\u001b[33mabstrak_id_filtered\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mabstrak_id_stemmed\u001b[39m\u001b[33m\"\u001b[39m]].head(\u001b[32m10\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\series.py:4924\u001b[39m, in \u001b[36mSeries.apply\u001b[39m\u001b[34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[39m\n\u001b[32m   4789\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapply\u001b[39m(\n\u001b[32m   4790\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   4791\u001b[39m     func: AggFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m   4796\u001b[39m     **kwargs,\n\u001b[32m   4797\u001b[39m ) -> DataFrame | Series:\n\u001b[32m   4798\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4799\u001b[39m \u001b[33;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[32m   4800\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   4915\u001b[39m \u001b[33;03m    dtype: float64\u001b[39;00m\n\u001b[32m   4916\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   4917\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4918\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   4919\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4920\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4921\u001b[39m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m=\u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4922\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4923\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m-> \u001b[39m\u001b[32m4924\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\apply.py:1427\u001b[39m, in \u001b[36mSeriesApply.apply\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1424\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.apply_compat()\n\u001b[32m   1426\u001b[39m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1427\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\apply.py:1507\u001b[39m, in \u001b[36mSeriesApply.apply_standard\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1501\u001b[39m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[32m   1502\u001b[39m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[32m   1503\u001b[39m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[32m   1504\u001b[39m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[32m   1505\u001b[39m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[32m   1506\u001b[39m action = \u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj.dtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1507\u001b[39m mapped = \u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1508\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[32m   1509\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1511\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[32m0\u001b[39m], ABCSeries):\n\u001b[32m   1512\u001b[39m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[32m   1513\u001b[39m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[32m   1514\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj._constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index=obj.index)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\base.py:921\u001b[39m, in \u001b[36mIndexOpsMixin._map_values\u001b[39m\u001b[34m(self, mapper, na_action, convert)\u001b[39m\n\u001b[32m    918\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[32m    919\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m arr.map(mapper, na_action=na_action)\n\u001b[32m--> \u001b[39m\u001b[32m921\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[39m, in \u001b[36mmap_array\u001b[39m\u001b[34m(arr, mapper, na_action, convert)\u001b[39m\n\u001b[32m   1741\u001b[39m values = arr.astype(\u001b[38;5;28mobject\u001b[39m, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1743\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1745\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m lib.map_infer_mask(\n\u001b[32m   1746\u001b[39m         values, mapper, mask=isna(values).view(np.uint8), convert=convert\n\u001b[32m   1747\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mlib.pyx:2972\u001b[39m, in \u001b[36mpandas._libs.lib.map_infer\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 10\u001b[39m, in \u001b[36m<lambda>\u001b[39m\u001b[34m(tokens)\u001b[39m\n\u001b[32m      6\u001b[39m factory = StemmerFactory()\n\u001b[32m      7\u001b[39m indo_stemmer = factory.create_stemmer()\n\u001b[32m      9\u001b[39m pta_df[\u001b[33m\"\u001b[39m\u001b[33mabstrak_id_stemmed\u001b[39m\u001b[33m\"\u001b[39m] = pta_df[\u001b[33m\"\u001b[39m\u001b[33mabstrak_id_filtered\u001b[39m\u001b[33m\"\u001b[39m].apply(\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m     \u001b[38;5;28;01mlambda\u001b[39;00m tokens: \u001b[43m[\u001b[49m\u001b[43mindo_stemmer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mword\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     11\u001b[39m )\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mPTA - Stemming & Lemmatization (abstrak_id):\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     14\u001b[39m pta_df[[\u001b[33m\"\u001b[39m\u001b[33mabstrak_id_filtered\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mabstrak_id_stemmed\u001b[39m\u001b[33m\"\u001b[39m]].head(\u001b[32m10\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 10\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m      6\u001b[39m factory = StemmerFactory()\n\u001b[32m      7\u001b[39m indo_stemmer = factory.create_stemmer()\n\u001b[32m      9\u001b[39m pta_df[\u001b[33m\"\u001b[39m\u001b[33mabstrak_id_stemmed\u001b[39m\u001b[33m\"\u001b[39m] = pta_df[\u001b[33m\"\u001b[39m\u001b[33mabstrak_id_filtered\u001b[39m\u001b[33m\"\u001b[39m].apply(\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m     \u001b[38;5;28;01mlambda\u001b[39;00m tokens: [\u001b[43mindo_stemmer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m tokens]\n\u001b[32m     11\u001b[39m )\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mPTA - Stemming & Lemmatization (abstrak_id):\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     14\u001b[39m pta_df[[\u001b[33m\"\u001b[39m\u001b[33mabstrak_id_filtered\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mabstrak_id_stemmed\u001b[39m\u001b[33m\"\u001b[39m]].head(\u001b[32m10\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\Sastrawi\\Stemmer\\CachedStemmer.py:20\u001b[39m, in \u001b[36mCachedStemmer.stem\u001b[39m\u001b[34m(self, text)\u001b[39m\n\u001b[32m     18\u001b[39m     stems.append(\u001b[38;5;28mself\u001b[39m.cache.get(word))\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m     stem = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdelegatedStemmer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m     \u001b[38;5;28mself\u001b[39m.cache.set(word, stem)\n\u001b[32m     22\u001b[39m     stems.append(stem)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\Sastrawi\\Stemmer\\Stemmer.py:27\u001b[39m, in \u001b[36mStemmer.stem\u001b[39m\u001b[34m(self, text)\u001b[39m\n\u001b[32m     24\u001b[39m stems = []\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words:\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m     stems.append(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstem_word\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m.join(stems)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\Sastrawi\\Stemmer\\Stemmer.py:36\u001b[39m, in \u001b[36mStemmer.stem_word\u001b[39m\u001b[34m(self, word)\u001b[39m\n\u001b[32m     34\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stem_plural_word(word)\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstem_singular_word\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\Sastrawi\\Stemmer\\Stemmer.py:84\u001b[39m, in \u001b[36mStemmer.stem_singular_word\u001b[39m\u001b[34m(self, word)\u001b[39m\n\u001b[32m     82\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Stem a singular word to its common stem form.\"\"\"\u001b[39;00m\n\u001b[32m     83\u001b[39m context = Context(word, \u001b[38;5;28mself\u001b[39m.dictionary, \u001b[38;5;28mself\u001b[39m.visitor_provider)\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     86\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m context.result\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\Sastrawi\\Stemmer\\Context\\Context.py:37\u001b[39m, in \u001b[36mContext.execute\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     34\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Execute stemming process; the result can be retrieved with result\"\"\"\u001b[39;00m\n\u001b[32m     36\u001b[39m \u001b[38;5;66;03m#step 1 - 5\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstart_stemming_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m#step 6\u001b[39;00m\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dictionary.contains(\u001b[38;5;28mself\u001b[39m.current_word):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\Sastrawi\\Stemmer\\Context\\Context.py:80\u001b[39m, in \u001b[36mContext.start_stemming_process\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     77\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m     79\u001b[39m \u001b[38;5;66;03m#step 4, 5\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mremove_prefixes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dictionary.contains(\u001b[38;5;28mself\u001b[39m.current_word):\n\u001b[32m     82\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\Sastrawi\\Stemmer\\Context\\Context.py:89\u001b[39m, in \u001b[36mContext.remove_prefixes\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     87\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mremove_prefixes\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m     88\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m3\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m89\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maccept_prefix_visitors\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprefix_pisitors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     90\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dictionary.contains(\u001b[38;5;28mself\u001b[39m.current_word):\n\u001b[32m     91\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\Sastrawi\\Stemmer\\Context\\Context.py:110\u001b[39m, in \u001b[36mContext.accept_prefix_visitors\u001b[39m\u001b[34m(self, visitors)\u001b[39m\n\u001b[32m    108\u001b[39m removalCount = \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.removals)\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m visitor \u001b[38;5;129;01min\u001b[39;00m visitors:\n\u001b[32m--> \u001b[39m\u001b[32m110\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maccept\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvisitor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    111\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dictionary.contains(\u001b[38;5;28mself\u001b[39m.current_word):\n\u001b[32m    112\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.current_word\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\Sastrawi\\Stemmer\\Context\\Context.py:97\u001b[39m, in \u001b[36mContext.accept\u001b[39m\u001b[34m(self, visitor)\u001b[39m\n\u001b[32m     96\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34maccept\u001b[39m(\u001b[38;5;28mself\u001b[39m, visitor):\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m     \u001b[43mvisitor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvisit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\Sastrawi\\Stemmer\\Context\\Visitor\\AbstractDisambiguatePrefixRule.py:14\u001b[39m, in \u001b[36mAbstractDisambiguatePrefixRule.visit\u001b[39m\u001b[34m(self, context)\u001b[39m\n\u001b[32m     11\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m disambiguator \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.disambiguators:\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     result = \u001b[43mdisambiguator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdisambiguate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcurrent_word\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m context.dictionary.contains(result):\n\u001b[32m     16\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\Sastrawi\\Morphology\\Disambiguator\\DisambiguatorPrefixRule15.py:21\u001b[39m, in \u001b[36mDisambiguatorPrefixRule15b.disambiguate\u001b[39m\u001b[34m(self, word)\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mDisambiguatorPrefixRule15b\u001b[39;00m(\u001b[38;5;28mobject\u001b[39m):\n\u001b[32m     17\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Disambiguate Prefix Rule 15b\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[33;03m    Rule 15b : men{V} -> me-t{V}\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdisambiguate\u001b[39m(\u001b[38;5;28mself\u001b[39m, word):\n\u001b[32m     22\u001b[39m \u001b[38;5;250m        \u001b[39m\u001b[33;03m\"\"\"Disambiguate Prefix Rule 15b\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[33;03m        Rule 15b : men{V} -> me-t{V}\u001b[39;00m\n\u001b[32m     24\u001b[39m \u001b[33;03m        \"\"\"\u001b[39;00m\n\u001b[32m     25\u001b[39m         matches = re.match(\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m\u001b[33m^men([aiueo])(.*)$\u001b[39m\u001b[33m'\u001b[39m, word)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Install Sastrawi untuk stemming bahasa Indonesia\n",
    "%pip install Sastrawi\n",
    "\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "\n",
    "factory = StemmerFactory()\n",
    "indo_stemmer = factory.create_stemmer()\n",
    "\n",
    "pta_df[\"abstrak_id_stemmed\"] = pta_df[\"abstrak_id_filtered\"].apply(\n",
    "    lambda tokens: [indo_stemmer.stem(word) for word in tokens]\n",
    ")\n",
    "\n",
    "print(\"\\nPTA - Stemming & Lemmatization (abstrak_id):\")\n",
    "pta_df[[\"abstrak_id_filtered\", \"abstrak_id_stemmed\"]].head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hFVjDTt3OjhP"
   },
   "source": [
    "#Menangani Kontraksi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nlw2mi-YO_Lv",
    "outputId": "f80af46d-3e98-489d-f688-4c82d34d4587"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: contractions in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.1.73)\n",
      "Requirement already satisfied: textsearch>=0.0.21 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from contractions) (0.0.24)\n",
      "Requirement already satisfied: anyascii in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from textsearch>=0.0.21->contractions) (0.3.3)\n",
      "Requirement already satisfied: pyahocorasick in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from textsearch>=0.0.21->contractions) (2.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install contractions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Kontraksi bahasa indonesia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Penanganan kontraksi selesai untuk 1,031 abstrak\n",
      "\n",
      "üìä HASIL PENANGANAN KONTRAKSI (SEMUA DATA):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstrak_stemmed</th>\n",
       "      <th>abstrak_expanded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>satiyah pengaruh faktorfaktor latih kembang pr...</td>\n",
       "      <td>satiyah pengaruh faktor latih kembang produkti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tuju teliti persepsi brand association langgan...</td>\n",
       "      <td>tuju teliti persepsi brand association langgan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aplikasi nyata manfaat teknologi informasi kom...</td>\n",
       "      <td>aplikasi nyata manfaat teknologi informasi kom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>teliti metode kuantitatif tekan uji hipotesis ...</td>\n",
       "      <td>teliti metode kuantitatif tekan uji hipotesis ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1026</th>\n",
       "      <td>teliti tuju hitung tingkat huni kamar hotel pa...</td>\n",
       "      <td>teliti tuju hitung tingkat huni kamar hotel pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1027</th>\n",
       "      <td>uswatun hasanah pengaruh latih kompensasi prod...</td>\n",
       "      <td>uswatun hasanah pengaruh latih kompensasi prod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1028</th>\n",
       "      <td>tuju teliti peran service performance climate ...</td>\n",
       "      <td>tuju teliti peran service performance climate ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1029</th>\n",
       "      <td>teliti tuju baur promosi iklan x jual pribadi ...</td>\n",
       "      <td>teliti tuju baur promosi iklan x jual pribadi ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1030</th>\n",
       "      <td>teliti tuju pengaruh motivasi kerja mediator s...</td>\n",
       "      <td>teliti tuju pengaruh motivasi kerja mediator s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1031 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        abstrak_stemmed  \\\n",
       "0     satiyah pengaruh faktorfaktor latih kembang pr...   \n",
       "1     tuju teliti persepsi brand association langgan...   \n",
       "2                                                         \n",
       "3     aplikasi nyata manfaat teknologi informasi kom...   \n",
       "4     teliti metode kuantitatif tekan uji hipotesis ...   \n",
       "...                                                 ...   \n",
       "1026  teliti tuju hitung tingkat huni kamar hotel pa...   \n",
       "1027  uswatun hasanah pengaruh latih kompensasi prod...   \n",
       "1028  tuju teliti peran service performance climate ...   \n",
       "1029  teliti tuju baur promosi iklan x jual pribadi ...   \n",
       "1030  teliti tuju pengaruh motivasi kerja mediator s...   \n",
       "\n",
       "                                       abstrak_expanded  \n",
       "0     satiyah pengaruh faktor latih kembang produkti...  \n",
       "1     tuju teliti persepsi brand association langgan...  \n",
       "2                                                        \n",
       "3     aplikasi nyata manfaat teknologi informasi kom...  \n",
       "4     teliti metode kuantitatif tekan uji hipotesis ...  \n",
       "...                                                 ...  \n",
       "1026  teliti tuju hitung tingkat huni kamar hotel pa...  \n",
       "1027  uswatun hasanah pengaruh latih kompensasi prod...  \n",
       "1028  tuju teliti peran service performance climate ...  \n",
       "1029  teliti tuju baur promosi iklan x jual pribadi ...  \n",
       "1030  teliti tuju pengaruh motivasi kerja mediator s...  \n",
       "\n",
       "[1031 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def expand_indonesian_contractions(text):\n",
    "    \"\"\"\n",
    "    Fungsi untuk memperluas bentuk slang atau kontraksi informal bahasa Indonesia\n",
    "    menjadi bentuk baku.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Teks input dalam bahasa Indonesia.\n",
    "    \n",
    "    Returns:\n",
    "        str: Teks dengan kontraksi/slang yang telah diperluas.\n",
    "    \"\"\"\n",
    "    # Kamus kontraksi/slang bahasa Indonesia\n",
    "    contractions_dict = {\n",
    "        \"gak\": \"tidak\", \"ga\": \"tidak\", \"nggak\": \"tidak\", \"enggak\": \"tidak\", \"ngga\": \"tidak\", \"gk\": \"tidak\", \"tdk\": \"tidak\", \"tk\": \"tidak\",\n",
    "      \"gue\": \"saya\", \"gw\": \"saya\", \"gua\": \"saya\", \"sy\": \"saya\", \"aq\": \"saya\", \"q\": \"saya\", \"ane\": \"saya\",\n",
    "      \"lu\": \"kamu\", \"loe\": \"kamu\", \"lo\": \"kamu\", \"km\": \"kamu\", \"kmu\": \"kamu\", \"elu\": \"kamu\",\n",
    "      \"dah\": \"sudah\", \"udah\": \"sudah\", \"sdh\": \"sudah\", \"udh\": \"sudah\",\n",
    "      \"blm\": \"belum\", \"td\": \"tadi\", \"ntar\": \"nanti\", \"skr\": \"sekarang\", \"skrg\": \"sekarang\", \"skg\": \"sekarang\",\n",
    "      \"kmrn\": \"kemarin\", \"kemrn\": \"kemarin\", \"kmarin\": \"kemarin\",\n",
    "      \"aja\": \"saja\", \"aj\": \"saja\", \"sj\": \"saja\",\n",
    "      \"nih\": \"ini\", \"nie\": \"ini\", \"ni\": \"ini\", \"tuh\": \"itu\", \"gtu\": \"begitu\", \"gitu\": \"begitu\",\n",
    "      \"trs\": \"terus\", \"trus\": \"terus\",\n",
    "      \"yg\": \"yang\", \"utk\": \"untuk\", \"dlm\": \"dalam\", \"dr\": \"dari\", \"dg\": \"dengan\", \"jd\": \"jadi\", \"jg\": \"juga\",\n",
    "      \"krn\": \"karena\", \"tp\": \"tetapi\", \"tpi\": \"tetapi\", \"sm\": \"sama\", \"thd\": \"terhadap\",\n",
    "      \"dll\": \"dan lain-lain\", \"dsb\": \"dan sebagainya\", \"dst\": \"dan seterusnya\",\n",
    "      \"banget\": \"sekali\", \"bgt\": \"sekali\", \"sgt\": \"sangat\", \"sngt\": \"sangat\",\n",
    "      \"lg\": \"sedang\", \"sdg\": \"sedang\",\n",
    "      \"dl\": \"dulu\", \"pls\": \"tolong\", \"tolongin\": \"tolong\", \"plis\": \"tolong\",\n",
    "      \"wkwk\": \"tertawa\", \"wkwkwk\": \"tertawa\", \"hehe\": \"tertawa kecil\", \"hihi\": \"tertawa kecil\",\n",
    "      \"btw\": \"ngomong-ngomong\", \"imo\": \"menurut saya\", \"imho\": \"menurut saya\", \"cmiiw\": \"koreksi jika saya salah\",\n",
    "      \"idk\": \"saya tidak tahu\", \"jk\": \"hanya bercanda\",\n",
    "      \"ok\": \"baik\", \"oke\": \"baik\", \"okey\": \"baik\", \"sip\": \"baik\",\n",
    "      \"ciyus\": \"serius\", \"serem\": \"menyeramkan\",\n",
    "      \"kl\": \"kalau\", \"klo\": \"kalau\", \"klu\": \"kalau\",\n",
    "      \"spy\": \"supaya\", \"spya\": \"supaya\",\n",
    "      \"bbrp\": \"beberapa\", \"tsb\": \"tersebut\", \"trsbt\": \"tersebut\",\n",
    "      \"dpt\": \"dapat\", \"bs\": \"bisa\", \"bsa\": \"bisa\",\n",
    "      \"stlh\": \"setelah\", \"sblm\": \"sebelum\",\n",
    "\n",
    "      \"mnj\": \"manajemen\", \"man\": \"manajemen\", \"mgt\": \"management\",\n",
    "      \"org\": \"organisasi\", \"org2\": \"organisasi\", \"orgzt\": \"organisasi\",\n",
    "      \"str\": \"struktur\", \"stkt\": \"struktur\",\n",
    "      \"ldr\": \"leader\", \"ldrshp\": \"leadership\", \"pimp\": \"pimpinan\", \"pemimp\": \"pemimpin\",\n",
    "      \"pln\": \"perencanaan\", \"renc\": \"perencanaan\", \"plan\": \"perencanaan\",\n",
    "      \"orgz\": \"organizing\", \"orgzn\": \"organisasi\",\n",
    "      \"dir\": \"directing\", \"pgn\": \"pengarahan\",\n",
    "      \"cnt\": \"control\", \"cont\": \"control\", \"ctrl\": \"kontrol\", \"pengend\": \"pengendalian\",\n",
    "      \"eff\": \"efisiensi\", \"effct\": \"efektivitas\",\n",
    "      \"sdm\": \"sumber daya manusia\", \"hr\": \"human resource\", \"hrd\": \"human resource development\",\n",
    "      \"res\": \"resource\", \"rsc\": \"resource\", \"sda\": \"sumber daya alam\",\n",
    "      \"inv\": \"investasi\", \"invt\": \"investasi\",\n",
    "      \"pmas\": \"pemasaran\", \"mkt\": \"marketing\", \"mktg\": \"marketing\",\n",
    "      \"prod\": \"produksi\", \"prdks\": \"produksi\",\n",
    "      \"fin\": \"finance\", \"keu\": \"keuangan\", \"akut\": \"akuntansi\", \"acct\": \"akuntansi\",\n",
    "      \"ris\": \"risiko\", \"rsko\": \"risiko\",\n",
    "      \"anal\": \"analisis\", \"eval\": \"evaluasi\",\n",
    "      \"strtg\": \"strategi\", \"stg\": \"strategi\",\n",
    "      \"ops\": \"operasi\", \"opr\": \"operasional\", \"oprs\": \"operasional\",\n",
    "      \"bsc\": \"balanced scorecard\", \"swot\": \"analisis swot\", \"pest\": \"analisis pest\",\n",
    "      \"csr\": \"corporate social responsibility\", \"gcn\": \"good corporate governance\",\n",
    "      \"qm\": \"quality management\", \"iso\": \"standar iso\",\n",
    "      \"kpi\": \"key performance indicator\", \"indik\": \"indikator\",\n",
    "      \"knowl\": \"knowledge management\", \"kmgt\": \"knowledge management\",\n",
    "      \"chg\": \"change management\", \"innv\": \"inovasi\",\n",
    "      \"cnfl\": \"konflik\", \"cnflt\": \"konflik\",\n",
    "      \"krj\": \"kerja\", \"tm\": \"tim\", \"tmwrk\": \"kerja sama tim\",\n",
    "      \"cst\": \"cost\", \"faktorfaktor\": \"faktor\", \"dampakdampak\": \"dampak\", \"bya\": \"biaya\",\n",
    "      \"val\": \"nilai\", \"valu\": \"value\",\n",
    "      \"proj\": \"proyek\", \"prjk\": \"proyek\",\n",
    "\n",
    "      \"med\": \"medis\", \"obat2\": \"obat-obatan\", \"rs\": \"rumah sakit\",\n",
    "      \"dok\": \"dokter\", \"drg\": \"dokter gigi\", \"prof\": \"profesor\",\n",
    "      \"pt\": \"perguruan tinggi\", \"univ\": \"universitas\", \"fak\": \"fakultas\",\n",
    "      \"skripsi\": \"skripsi\", \"tesis\": \"tesis\", \"disertasi\": \"disertasi\",\n",
    "      \"mhs\": \"mahasiswa\", \"mhsw\": \"mahasiswa\"\n",
    "    }\n",
    "    \n",
    "    # Membuat pola regex untuk mencocokkan kata-kata utuh\n",
    "    pattern = r'\\b(' + '|'.join(re.escape(key) for key in contractions_dict.keys()) + r')\\b'\n",
    "    \n",
    "    # Fungsi pengganti (DIPERBAIKI)\n",
    "    def replace_match(match):\n",
    "        original_word = match.group(0)  # Ambil kata asli (misal: 'Gue')\n",
    "        lower_word = original_word.lower()  # Konversi ke huruf kecil (misal: 'gue')\n",
    "        return contractions_dict[lower_word]  # Cari di kamus\n",
    "    \n",
    "    # Lakukan penggantian\n",
    "    expanded_text = re.sub(pattern, replace_match, text, flags=re.IGNORECASE)\n",
    "    \n",
    "    return expanded_text\n",
    "\n",
    "# Terapkan kontraksi bahasa Indonesia ke data abstrak yang sudah di-stemming\n",
    "pta_df[\"abstrak_id_expanded\"] = pta_df[\"abstrak_id_stemmed\"].apply(\n",
    "    lambda tokens: expand_indonesian_contractions(\" \".join(tokens)).split()\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Penanganan kontraksi selesai untuk {len(pta_df):,} abstrak\")\n",
    "\n",
    "# Tampilkan hasil dalam bentuk DataFrame\n",
    "print(\"\\nüìä HASIL PENANGANAN KONTRAKSI (SEMUA DATA):\")\n",
    "comparison_df = pd.DataFrame({\n",
    "    'abstrak_stemmed': pta_df[\"abstrak_id_stemmed\"].apply(lambda x: \" \".join(x[:15]) + \"...\" if len(x) > 15 else \" \".join(x)),\n",
    "    'abstrak_expanded': pta_df[\"abstrak_id_expanded\"].apply(lambda x: \" \".join(x[:15]) + \"...\" if len(x) > 15 else \" \".join(x))\n",
    "})\n",
    "\n",
    "comparison_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspellchecker in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.8.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\hp\\appdata\\roaming\\python\\python311\\site-packages (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Working directory saat ini: d:\\kuliah\\semester 7\\ppw\\tugas_ppw\\tugas_4\n",
      "‚úÖ File ditemukan: d:\\kuliah\\semester 7\\ppw\\tugas_ppw\\tugas_4\\00-indonesian-wordlist.lst\n",
      "‚úÖ Berhasil memuat 79,898 kata dari kamus Indonesia\n",
      "üìä DataFrame ditemukan dengan 1031 dokumen\n",
      "üîç Kolom yang tersedia: ['abstrak_id_clean', 'abstrak_id_tokens', 'abstrak_id_filtered']\n",
      "‚ùå Kolom 'abstrak_id_expanded' tidak ditemukan!\n",
      "üí° Jalankan sel 'Kontraksi bahasa indonesia' terlebih dahulu\n",
      "üîß Kolom alternatif yang tersedia: ['abstrak_id_clean', 'abstrak_id_tokens', 'abstrak_id_filtered']\n",
      "üîÑ Menggunakan kolom 'abstrak_id_filtered' sebagai gantinya...\n",
      "üìä Memproses 1031 dokumen...\n",
      "‚úÖ Berhasil memuat 79,898 kata dari kamus Indonesia\n",
      "üìä DataFrame ditemukan dengan 1031 dokumen\n",
      "üîç Kolom yang tersedia: ['abstrak_id_clean', 'abstrak_id_tokens', 'abstrak_id_filtered']\n",
      "‚ùå Kolom 'abstrak_id_expanded' tidak ditemukan!\n",
      "üí° Jalankan sel 'Kontraksi bahasa indonesia' terlebih dahulu\n",
      "üîß Kolom alternatif yang tersedia: ['abstrak_id_clean', 'abstrak_id_tokens', 'abstrak_id_filtered']\n",
      "üîÑ Menggunakan kolom 'abstrak_id_filtered' sebagai gantinya...\n",
      "üìä Memproses 1031 dokumen...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Spellchecking:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 613/1031 [1:50:35<1:23:17, 11.96s/row]"
     ]
    }
   ],
   "source": [
    "# Install library yang diperlukan\n",
    "%pip install pyspellchecker tqdm\n",
    "\n",
    "from spellchecker import SpellChecker\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# Inisialisasi SpellChecker kosong\n",
    "spell = SpellChecker(language=None)\n",
    "\n",
    "# Load kamus Indonesia dari file\n",
    "wordlist_file = \"00-indonesian-wordlist.lst\"\n",
    "\n",
    "if os.path.exists(wordlist_file):\n",
    "    with open(wordlist_file, \"r\", encoding=\"latin-1\") as f:\n",
    "        indo_words = [line.strip() for line in f.readlines()]\n",
    "    \n",
    "    spell.word_frequency.load_words(indo_words)\n",
    "    print(f\"‚úÖ Memuat {len(indo_words):,} kata dari kamus Indonesia\")\n",
    "else:\n",
    "    # Cari file di direktori lain yang mungkin\n",
    "    possible_paths = [\n",
    "        os.path.join(\"tugas_4\", wordlist_file),\n",
    "        os.path.join(\"..\", \"tugas_4\", wordlist_file),\n",
    "    ]\n",
    "    \n",
    "    for path in possible_paths:\n",
    "        if os.path.exists(path):\n",
    "            wordlist_file = path\n",
    "            break\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"File '{wordlist_file}' tidak ditemukan!\")\n",
    "    \n",
    "    with open(wordlist_file, \"r\", encoding=\"latin-1\") as f:\n",
    "        indo_words = [line.strip() for line in f.readlines()]\n",
    "    \n",
    "    spell.word_frequency.load_words(indo_words)\n",
    "    print(f\"‚úÖ Memuat {len(indo_words):,} kata dari kamus Indonesia\")\n",
    "\n",
    "# Fungsi untuk koreksi kata\n",
    "def correct_word(word):\n",
    "    if not word or len(word) < 2:\n",
    "        return word\n",
    "    corr = spell.correction(word)\n",
    "    return corr if corr is not None else word\n",
    "\n",
    "# Pastikan pta_df sudah ada dan memiliki kolom yang diperlukan\n",
    "if 'pta_df' in locals() and not pta_df.empty:\n",
    "    # Tentukan kolom sumber yang akan digunakan\n",
    "    if 'abstrak_id_expanded' in pta_df.columns:\n",
    "        source_column = 'abstrak_id_expanded'\n",
    "    else:\n",
    "        # Gunakan kolom alternatif\n",
    "        available_columns = [col for col in pta_df.columns if 'abstrak' in col.lower()]\n",
    "        source_column = available_columns[-1] if available_columns else None\n",
    "        print(f\"‚ö†Ô∏è  Menggunakan '{source_column}' karena 'abstrak_id_expanded' belum tersedia\")\n",
    "    \n",
    "    if source_column:\n",
    "        print(f\"üî§ Memproses spell checking untuk {len(pta_df)} dokumen...\")\n",
    "        \n",
    "        # Terapkan spellcheck\n",
    "        corrected_texts = []\n",
    "        for tokens in tqdm(pta_df[source_column], desc=\"Spell checking\", unit=\"row\"):\n",
    "            if isinstance(tokens, str):\n",
    "                tokens = tokens.split()\n",
    "            elif not isinstance(tokens, list):\n",
    "                tokens = []\n",
    "            \n",
    "            corrected = [correct_word(word) for word in tokens if word]\n",
    "            corrected_texts.append(corrected)\n",
    "\n",
    "        pta_df[\"abstrak_id_spellchecked\"] = corrected_texts\n",
    "        print(\"‚úÖ Spell checking selesai!\")\n",
    "        \n",
    "        # Tampilkan contoh hasil (hanya 3 dokumen)\n",
    "        print(\"\\nüìù Contoh hasil (3 dokumen pertama):\")\n",
    "        for i in range(min(3, len(pta_df))):\n",
    "            original_tokens = pta_df[source_column].iloc[i]\n",
    "            if isinstance(original_tokens, str):\n",
    "                original_tokens = original_tokens.split()\n",
    "            elif not isinstance(original_tokens, list):\n",
    "                original_tokens = []\n",
    "                \n",
    "            corrected_tokens = pta_df[\"abstrak_id_spellchecked\"].iloc[i]\n",
    "            \n",
    "            original = \" \".join(original_tokens[:8])\n",
    "            corrected = \" \".join(corrected_tokens[:8])\n",
    "            print(f\"Dok {i+1}: {original}... ‚Üí {corrected}...\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå DataFrame 'pta_df' tidak ditemukan!\")\n",
    "    print(\"üí° Jalankan sel sebelumnya terlebih dahulu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Le5IOQAOvTI"
   },
   "source": [
    "# Pemeriksaan Ejaan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fbJq8d5TPH-I",
    "outputId": "fe306003-9989-4f22-ac96-342f545cf4a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspellchecker in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.8.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install pyspellchecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7nKpz_UMOwoo",
    "outputId": "486b23ee-d15d-4e0b-d057-481dd1dfabe5"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenized_corpus' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mspellchecker\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SpellChecker\n\u001b[32m      3\u001b[39m spell = SpellChecker()\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m corrected_corpus = [[spell.correction(word) \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m doc] \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtokenized_corpus\u001b[49m]\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(corrected_corpus)\n",
      "\u001b[31mNameError\u001b[39m: name 'tokenized_corpus' is not defined"
     ]
    }
   ],
   "source": [
    "from spellchecker import SpellChecker\n",
    "\n",
    "spell = SpellChecker()\n",
    "corrected_corpus = [[spell.correction(word) for word in doc] for doc in pta_df[\"abstrak_id_expanded\"]]\n",
    "print(corrected_corpus)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemmed (Hasil Stemming)\n",
    "Stemming adalah proses mengembalikan kata ke bentuk dasarnya dengan memotong imbuhan (awalan, akhiran, sisipan).\n",
    "\n",
    "Contoh:\n",
    "\n",
    "\"berjalan\" ‚Üí \"jalan\"\n",
    "\"berlari\" ‚Üí \"lari\"\n",
    "\"menjalankan\" ‚Üí \"jalan\"\n",
    "\"pembelajaran\" ‚Üí \"ajar\"\n",
    "\n",
    "Expanded (Hasil Penanganan Kontraksi)\n",
    "Expansion adalah proses mengubah kata-kata singkatan atau slang menjadi bentuk lengkapnya.\n",
    "\n",
    "Contoh:\n",
    "\n",
    "\"gak\" ‚Üí \"tidak\"\n",
    "\"yg\" ‚Üí \"yang\"\n",
    "\"utk\" ‚Üí \"untuk\"\n",
    "\"dr\" ‚Üí \"dari\"\n",
    "\"tp\" ‚Üí \"tetapi\"\n",
    "\n",
    "Urutan Proses dalam Pipeline:\n",
    "Teks Asli: \"Penelitian ini menggunakan metode yg tepat utk analisis data\"\n",
    "Cleaning: \"penelitian ini menggunakan metode yg tepat utk analisis data\"\n",
    "Tokenizing: [\"penelitian\", \"ini\", \"menggunakan\", \"metode\", \"yg\", \"tepat\", \"utk\", \"analisis\", \"data\"]\n",
    "Stop Words Removal: [\"penelitian\", \"menggunakan\", \"metode\", \"yg\", \"tepat\", \"utk\", \"analisis\", \"data\"]\n",
    "Stemming: [\"teliti\", \"guna\", \"metode\", \"yg\", \"tepat\", \"utk\", \"analisis\", \"data\"] ‚Üê INI STEMMED\n",
    "Expansion: [\"teliti\", \"guna\", \"metode\", \"yang\", \"tepat\", \"untuk\", \"analisis\", \"data\"] ‚Üê INI EXPANDED\n",
    "\n",
    "Stemmed = kata sudah dipotong imbuhannya\n",
    "Expanded = singkatan/slang sudah diubah ke bentuk lengkap"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}