{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.32.5)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.13.5)\n",
      "Requirement already satisfied: lxml in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (6.0.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests) (2025.8.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from beautifulsoup4) (2.7)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from beautifulsoup4) (4.14.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openpyxl in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.1.5)\n",
      "Requirement already satisfied: xlsxwriter in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.2.9)\n",
      "Requirement already satisfied: et-xmlfile in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openpyxl) (2.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install requests beautifulsoup4 lxml\n",
    "%pip install openpyxl xlsxwriter\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "BASE_URL = \"https://pta.trunojoyo.ac.id/c_search/byprod\"\n",
    "\n",
    "def get_max_page(prodi_id):\n",
    "    \"\"\"Mendapatkan jumlah halaman maksimal untuk prodi tertentu\"\"\"\n",
    "    try:\n",
    "        url = f\"{BASE_URL}/{prodi_id}/1\"\n",
    "        response = requests.get(url, timeout=10)\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        \n",
    "        pagination = soup.select('a[href*=\"byprod\"]')\n",
    "        if pagination:\n",
    "            last_page = 1\n",
    "            for link in pagination:\n",
    "                href = link.get('href', '')\n",
    "                if f'/byprod/{prodi_id}/' in href:\n",
    "                    page_match = re.search(rf'/byprod/{prodi_id}/(\\d+)', href)\n",
    "                    if page_match:\n",
    "                        page_num = int(page_match.group(1))\n",
    "                        last_page = max(last_page, page_num)\n",
    "            return last_page\n",
    "        return 1\n",
    "    except:\n",
    "        return 1\n",
    "\n",
    "def pta():\n",
    "    \"\"\"Fungsi utama untuk scraping data PTA\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    data = {\n",
    "        \"judul\": [], \"abstrak_id\": [], \"abstrak_en\": []\n",
    "    }\n",
    "    \n",
    "    prodi_id = 7\n",
    "    prodi_name = \"Manajemen\"\n",
    "    \n",
    "    # Get max pages\n",
    "    max_page = get_max_page(prodi_id)\n",
    "    \n",
    "    for j in range(1, max_page + 1):\n",
    "        try:\n",
    "            url = f\"{BASE_URL}/{prodi_id}/{j}\"\n",
    "            r = requests.get(url, timeout=15)\n",
    "            soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "            jurnals = soup.select('li[data-cat=\"#luxury\"]')\n",
    "            \n",
    "            # Process setiap jurnal\n",
    "            for jurnal in jurnals:\n",
    "                try:\n",
    "                    link_keluar = jurnal.select_one('a.gray.button')['href']\n",
    "                    \n",
    "                    response = requests.get(link_keluar, timeout=15)\n",
    "                    soup1 = BeautifulSoup(response.content, \"html.parser\")\n",
    "                    isi = soup1.select_one('div#content_journal')\n",
    "                    \n",
    "                    if not isi:\n",
    "                        continue\n",
    "                    \n",
    "                    # Extract data\n",
    "                    judul = isi.select_one('a.title').text.strip()\n",
    "                    \n",
    "                    # Extract abstrak\n",
    "                    paragraf = isi.select('p[align=\"justify\"]')\n",
    "                    abstrak_id = paragraf[0].get_text(strip=True) if len(paragraf) > 0 else \"N/A\"\n",
    "                    abstrak_en = paragraf[1].get_text(strip=True) if len(paragraf) > 1 else \"N/A\"\n",
    "                    \n",
    "                    # Clean abstrak Indonesia\n",
    "                    if abstrak_id != \"N/A\" and abstrak_id.upper().startswith('ABSTRAK'):\n",
    "                        abstrak_id = abstrak_id[7:].strip()\n",
    "                    \n",
    "                    # Append data\n",
    "                    data[\"judul\"].append(judul)\n",
    "                    data[\"abstrak_id\"].append(abstrak_id)\n",
    "                    data[\"abstrak_en\"].append(abstrak_en)\n",
    "                    \n",
    "                    time.sleep(0.1)\n",
    "                    \n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    # Buat DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Summary\n",
    "    print(f\"📊 Total data dikumpulkan: {len(df):,}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Jalankan scraping\n",
    "result_df = pta()\n",
    "\n",
    "# Ambil abstrak Indonesia untuk corpus\n",
    "corpus = result_df[result_df['abstrak_id'] != 'N/A']['abstrak_id'].tolist()\n",
    "\n",
    "# Tampilkan dalam bentuk DataFrame (semua data)\n",
    "import pandas as pd\n",
    "df_sample = pd.DataFrame({\n",
    "    'No': range(1, len(corpus) + 1),\n",
    "    'Abstrak': corpus\n",
    "})\n",
    "\n",
    "df_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "11UFB9KvOQyP"
   },
   "source": [
    "### **1. Pembersihan Teks**\n",
    "\n",
    "**Pembersihan teks** adalah tahap preprocessing untuk menormalkan data teks agar konsisten dan siap dianalisis. Proses ini meliputi: mengubah teks ke huruf kecil (lowercasing), menghapus angka, menghilangkan tanda baca, dan membersihkan spasi berlebih. Tujuannya adalah mengurangi variasi yang tidak perlu dan memfokuskan analisis pada konten tekstual yang bermakna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hasil Pembersihan Teks:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstrak_asli</th>\n",
       "      <th>abstrak_bersih</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Satiyah, Pengaruh Faktor-faktor Pelatihan dan ...</td>\n",
       "      <td>satiyah pengaruh faktorfaktor pelatihan dan pe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tujuan penelitian ini adalah untuk mengetahui ...</td>\n",
       "      <td>tujuan penelitian ini adalah untuk mengetahui ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Aplikasi nyata pemanfaatan teknologi informasi...</td>\n",
       "      <td>aplikasi nyata pemanfaatan teknologi informasi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Penelitian ini menggunakan metode kuantitatif,...</td>\n",
       "      <td>penelitian ini menggunakan metode kuantitatif ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Aththaariq, Pengaruh Kompetensi Dosen Terhadap...</td>\n",
       "      <td>aththaariq pengaruh kompetensi dosen terhadap ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Haryono Arifin, Pengaruh Perilaku Konsumen Ter...</td>\n",
       "      <td>haryono arifin pengaruh perilaku konsumen terh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Dharma Abidin Syah,Kesimpulan: (1) Terdapat pe...</td>\n",
       "      <td>dharma abidin syahkesimpulan terdapat pengaruh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Tujuan penelitian ini adalah untuk mengidentif...</td>\n",
       "      <td>tujuan penelitian ini adalah untuk mengidentif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Hasil dari penelitian ini dari perhitungan Cre...</td>\n",
       "      <td>hasil dari penelitian ini dari perhitungan cre...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        abstrak_asli  \\\n",
       "0  Satiyah, Pengaruh Faktor-faktor Pelatihan dan ...   \n",
       "1  Tujuan penelitian ini adalah untuk mengetahui ...   \n",
       "2                                                      \n",
       "3  Aplikasi nyata pemanfaatan teknologi informasi...   \n",
       "4  Penelitian ini menggunakan metode kuantitatif,...   \n",
       "5  Aththaariq, Pengaruh Kompetensi Dosen Terhadap...   \n",
       "6  Haryono Arifin, Pengaruh Perilaku Konsumen Ter...   \n",
       "7  Dharma Abidin Syah,Kesimpulan: (1) Terdapat pe...   \n",
       "8  Tujuan penelitian ini adalah untuk mengidentif...   \n",
       "9  Hasil dari penelitian ini dari perhitungan Cre...   \n",
       "\n",
       "                                      abstrak_bersih  \n",
       "0  satiyah pengaruh faktorfaktor pelatihan dan pe...  \n",
       "1  tujuan penelitian ini adalah untuk mengetahui ...  \n",
       "2                                                     \n",
       "3  aplikasi nyata pemanfaatan teknologi informasi...  \n",
       "4  penelitian ini menggunakan metode kuantitatif ...  \n",
       "5  aththaariq pengaruh kompetensi dosen terhadap ...  \n",
       "6  haryono arifin pengaruh perilaku konsumen terh...  \n",
       "7  dharma abidin syahkesimpulan terdapat pengaruh...  \n",
       "8  tujuan penelitian ini adalah untuk mengidentif...  \n",
       "9  hasil dari penelitian ini dari perhitungan cre...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pembersihan teks\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def clean_text(text):\n",
    "    # Ubah ke huruf kecil\n",
    "    text = text.lower()\n",
    "    # Hapus angka\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # Hapus tanda baca\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Hapus spasi berlebih\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# Terapkan pembersihan teks\n",
    "cleaned_corpus = [clean_text(text) for text in corpus]\n",
    "\n",
    "# Tampilkan hasil pembersihan teks dalam format DataFrame\n",
    "comparison_df = pd.DataFrame({\n",
    "    'abstrak_asli': corpus[:10],\n",
    "    'abstrak_bersih': cleaned_corpus[:10]\n",
    "})\n",
    "\n",
    "print(\"\\nHasil Pembersihan Teks:\")\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k9j1pNkvOVKo"
   },
   "source": [
    "### **2. Tokenisasi**\n",
    "\n",
    "**Tokenisasi** adalah proses memecah teks menjadi unit-unit yang lebih kecil seperti kata atau token. Pada tahap ini, setiap kalimat dalam abstrak dipecah menjadi kata-kata individual menggunakan `word_tokenize` dari NLTK. Contoh: kalimat \"Penelitian ini menganalisis data\" → ['Penelitian', 'ini', 'menganalisis', 'data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yAJTtGY7OXCP",
    "outputId": "a22fe187-e3b5-4da4-a808-757a191de6e7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PTA (abstrak_id_tokens):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstrak_id_clean</th>\n",
       "      <th>abstrak_id_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>satiyah pengaruh faktorfaktor pelatihan dan pe...</td>\n",
       "      <td>[satiyah, pengaruh, faktorfaktor, pelatihan, d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tujuan penelitian ini adalah untuk mengetahui ...</td>\n",
       "      <td>[tujuan, penelitian, ini, adalah, untuk, menge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aplikasi nyata pemanfaatan teknologi informasi...</td>\n",
       "      <td>[aplikasi, nyata, pemanfaatan, teknologi, info...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>penelitian ini menggunakan metode kuantitatif ...</td>\n",
       "      <td>[penelitian, ini, menggunakan, metode, kuantit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>aththaariq pengaruh kompetensi dosen terhadap ...</td>\n",
       "      <td>[aththaariq, pengaruh, kompetensi, dosen, terh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>haryono arifin pengaruh perilaku konsumen terh...</td>\n",
       "      <td>[haryono, arifin, pengaruh, perilaku, konsumen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>dharma abidin syahkesimpulan terdapat pengaruh...</td>\n",
       "      <td>[dharma, abidin, syahkesimpulan, terdapat, pen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>tujuan penelitian ini adalah untuk mengidentif...</td>\n",
       "      <td>[tujuan, penelitian, ini, adalah, untuk, mengi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>hasil dari penelitian ini dari perhitungan cre...</td>\n",
       "      <td>[hasil, dari, penelitian, ini, dari, perhitung...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    abstrak_id_clean  \\\n",
       "0  satiyah pengaruh faktorfaktor pelatihan dan pe...   \n",
       "1  tujuan penelitian ini adalah untuk mengetahui ...   \n",
       "2                                                      \n",
       "3  aplikasi nyata pemanfaatan teknologi informasi...   \n",
       "4  penelitian ini menggunakan metode kuantitatif ...   \n",
       "5  aththaariq pengaruh kompetensi dosen terhadap ...   \n",
       "6  haryono arifin pengaruh perilaku konsumen terh...   \n",
       "7  dharma abidin syahkesimpulan terdapat pengaruh...   \n",
       "8  tujuan penelitian ini adalah untuk mengidentif...   \n",
       "9  hasil dari penelitian ini dari perhitungan cre...   \n",
       "\n",
       "                                   abstrak_id_tokens  \n",
       "0  [satiyah, pengaruh, faktorfaktor, pelatihan, d...  \n",
       "1  [tujuan, penelitian, ini, adalah, untuk, menge...  \n",
       "2                                                 []  \n",
       "3  [aplikasi, nyata, pemanfaatan, teknologi, info...  \n",
       "4  [penelitian, ini, menggunakan, metode, kuantit...  \n",
       "5  [aththaariq, pengaruh, kompetensi, dosen, terh...  \n",
       "6  [haryono, arifin, pengaruh, perilaku, konsumen...  \n",
       "7  [dharma, abidin, syahkesimpulan, terdapat, pen...  \n",
       "8  [tujuan, penelitian, ini, adalah, untuk, mengi...  \n",
       "9  [hasil, dari, penelitian, ini, dari, perhitung...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import pandas as pd\n",
    "\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# Tokenisasi untuk PTA\n",
    "# Buat DataFrame dari corpus yang sudah dibersihkan\n",
    "pta_df = pd.DataFrame({\n",
    "    'abstrak_id_clean': cleaned_corpus\n",
    "})\n",
    "\n",
    "pta_df[\"abstrak_id_tokens\"] = pta_df[\"abstrak_id_clean\"].apply(word_tokenize)\n",
    "\n",
    "print(\"\\nPTA (abstrak_id_tokens):\")\n",
    "pta_df[[\"abstrak_id_clean\", \"abstrak_id_tokens\"]].head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7MJMFTQVOarI"
   },
   "source": [
    "### **3. Penghapusan Kata Umum (Stop Words)**\n",
    "\n",
    "**Stop Words** adalah kata-kata umum yang sering muncul dalam bahasa namun tidak memberikan makna signifikan untuk analisis teks, seperti \"dan\", \"atau\", \"di\", \"ke\", \"yang\". Penghapusan stopwords bertujuan untuk mengurangi noise dan fokus pada kata-kata yang lebih bermakna untuk analisis. NLTK menyediakan daftar stopwords bahasa Indonesia yang sudah siap pakai."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PKkRL87EOben",
    "outputId": "775733a7-511b-447e-c341-f1c5b46a2274"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PTA (abstrak_id_filtered):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstrak_id_tokens</th>\n",
       "      <th>abstrak_id_filtered</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[satiyah, pengaruh, faktorfaktor, pelatihan, d...</td>\n",
       "      <td>[satiyah, pengaruh, faktorfaktor, pelatihan, p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[tujuan, penelitian, ini, adalah, untuk, menge...</td>\n",
       "      <td>[tujuan, penelitian, persepsi, brand, associat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[aplikasi, nyata, pemanfaatan, teknologi, info...</td>\n",
       "      <td>[aplikasi, nyata, pemanfaatan, teknologi, info...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[penelitian, ini, menggunakan, metode, kuantit...</td>\n",
       "      <td>[penelitian, metode, kuantitatif, menekankan, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[aththaariq, pengaruh, kompetensi, dosen, terh...</td>\n",
       "      <td>[aththaariq, pengaruh, kompetensi, dosen, kine...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[haryono, arifin, pengaruh, perilaku, konsumen...</td>\n",
       "      <td>[haryono, arifin, pengaruh, perilaku, konsumen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[dharma, abidin, syahkesimpulan, terdapat, pen...</td>\n",
       "      <td>[dharma, abidin, syahkesimpulan, pengaruh, sig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[tujuan, penelitian, ini, adalah, untuk, mengi...</td>\n",
       "      <td>[tujuan, penelitian, mengidentifikasi, variabe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[hasil, dari, penelitian, ini, dari, perhitung...</td>\n",
       "      <td>[hasil, penelitian, perhitungan, credit, risk,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   abstrak_id_tokens  \\\n",
       "0  [satiyah, pengaruh, faktorfaktor, pelatihan, d...   \n",
       "1  [tujuan, penelitian, ini, adalah, untuk, menge...   \n",
       "2                                                 []   \n",
       "3  [aplikasi, nyata, pemanfaatan, teknologi, info...   \n",
       "4  [penelitian, ini, menggunakan, metode, kuantit...   \n",
       "5  [aththaariq, pengaruh, kompetensi, dosen, terh...   \n",
       "6  [haryono, arifin, pengaruh, perilaku, konsumen...   \n",
       "7  [dharma, abidin, syahkesimpulan, terdapat, pen...   \n",
       "8  [tujuan, penelitian, ini, adalah, untuk, mengi...   \n",
       "9  [hasil, dari, penelitian, ini, dari, perhitung...   \n",
       "\n",
       "                                 abstrak_id_filtered  \n",
       "0  [satiyah, pengaruh, faktorfaktor, pelatihan, p...  \n",
       "1  [tujuan, penelitian, persepsi, brand, associat...  \n",
       "2                                                 []  \n",
       "3  [aplikasi, nyata, pemanfaatan, teknologi, info...  \n",
       "4  [penelitian, metode, kuantitatif, menekankan, ...  \n",
       "5  [aththaariq, pengaruh, kompetensi, dosen, kine...  \n",
       "6  [haryono, arifin, pengaruh, perilaku, konsumen...  \n",
       "7  [dharma, abidin, syahkesimpulan, pengaruh, sig...  \n",
       "8  [tujuan, penelitian, mengidentifikasi, variabe...  \n",
       "9  [hasil, penelitian, perhitungan, credit, risk,...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "# Download stopwords untuk bahasa Indonesia\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Stopwords untuk bahasa Indonesia\n",
    "stop_words_id = set(stopwords.words('indonesian'))\n",
    "\n",
    "# Filter stopwords di PTA\n",
    "pta_df[\"abstrak_id_filtered\"] = pta_df[\"abstrak_id_tokens\"].apply(\n",
    "    lambda tokens: [word for word in tokens if word not in stop_words_id]\n",
    ")\n",
    "\n",
    "print(\"\\nPTA (abstrak_id_filtered):\")\n",
    "pta_df[[\"abstrak_id_tokens\", \"abstrak_id_filtered\"]].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sCT0BB3ZOdMY"
   },
   "source": [
    "### **4. Stemming dan Lematisasi**\n",
    "\n",
    "**Stemming** adalah proses mengubah kata-kata menjadi bentuk dasar (root/stem) dengan menghilangkan imbuhan seperti awalan, akhiran, dan sisipan. Contoh: \"penelitian\" → \"teliti\", \"menganalisis\" → \"analisis\". **Lematisasi** serupa dengan stemming namun menghasilkan kata dasar yang lebih bermakna secara linguistik. Pada kode ini menggunakan library Sastrawi untuk stemming bahasa Indonesia dan TF-IDF untuk mengukur kepentingan kata dalam dokumen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x1Q44O9SOfwx",
    "outputId": "8b5d33a6-0d03-4c67-a75f-76f72719f2e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "satiyah pengaruh faktorfaktor latih kembang produktivitas kerja dinas laut ikan bangkal bawah bimbing drahjsanugrahini irawatimm helm buyung auliasstsemmt upaya tingkat produktivitas kerja mudah salah usaha produktivitas tingkat terap program latih kembang sumber daya manusia sdm laksana instansi produktivitas capai tingkat mampu pegawai efektif efisien latih pengembnagan harap pegawai sesuai kebutuhankebutuhan sikap tingkah laku terampil tahu sesuai tuntut ubah latih kembang pegawai dukung cipta suasana kerja kondusif instansi produktivitas kerja tingkat tuju teliti pengaruh faktorfaktor latih kembang produktivitas kerja dinas laut ikan bangkal ukur menganalisa hubung variabel teliti dekat observasional analitik amat langsung responden sebar kuisioner analis teliti teliti populasi responden sampel olah spss versi analis metode statistik metode non probality sampling simple random sampling simpul teliti a faktorfaktor latih kembang beda individu pegawai x hubung analisis jabat x motivasi x partisipasi aktif x seleksi serta x seleksi instruktur x metode latih kembang x pengaruh simultan produktivitas kerja pegawai dinas laut ikan kabupaten bangkal bukti nilai koefisien determinasi ganda r r square fhitung ftabel b faktor hubung analisis jabat pengaruh parsial produktivitas kerja pegawai dinas laut ikan kabupaten bangkal uji hipotesis variabel motivasi x bukti nilai thitung seleksi serta x pengaruh dominan produktivitas kerja pegawai instansi dinas laut ikan bangkal kunci dinas laut ikan faktorfaktor latih kembang produktivitas kerja\n",
      "  (0, 3835)\t0.06410079410983066\n",
      "  (0, 3127)\t0.05506979793696032\n",
      "  (0, 1234)\t0.14148728361531826\n",
      "  (0, 2320)\t0.35125220511909655\n",
      "  (0, 2025)\t0.2310024290789113\n",
      "  (0, 3380)\t0.4237295114125377\n",
      "  (0, 2052)\t0.15395801965547096\n",
      "  (0, 961)\t0.25423770684752256\n",
      "  (0, 2322)\t0.33272649654294295\n",
      "  (0, 1628)\t0.3224038704618687\n",
      "  (0, 433)\t0.1297471048665307\n",
      "  (0, 473)\t0.01717152469532511\n",
      "  (0, 570)\t0.014613258757647558\n",
      "  (0, 1030)\t0.057560061635212295\n",
      "  (0, 1794)\t0.05101932916059392\n",
      "  (0, 1521)\t0.03514978048749194\n",
      "  (0, 660)\t0.03559921220338679\n",
      "  (0, 375)\t0.06410079410983066\n",
      "  (0, 4636)\t0.045738636603348766\n",
      "  (0, 4486)\t0.07697830041873661\n",
      "  (0, 2741)\t0.04643794034963594\n",
      "  (0, 3787)\t0.029775504198206976\n",
      "  (0, 4645)\t0.01669403924897024\n",
      "  (0, 4418)\t0.04285696914744371\n",
      "  (0, 3399)\t0.03559921220338679\n",
      "  :\t:\n",
      "  (654, 3978)\t0.060610706972008\n",
      "  (654, 2974)\t0.121221413944016\n",
      "  (654, 1379)\t0.055431470737939546\n",
      "  (654, 1535)\t0.05393497462982479\n",
      "  (654, 1159)\t0.26110655802086774\n",
      "  (654, 2370)\t0.04500648366260221\n",
      "  (654, 4067)\t0.05802108885497378\n",
      "  (654, 715)\t0.058502612126048806\n",
      "  (654, 209)\t0.055831597934352994\n",
      "  (654, 1078)\t0.041648917650643866\n",
      "  (654, 791)\t0.2580347664411713\n",
      "  (654, 3322)\t0.055831597934352994\n",
      "  (654, 2277)\t0.060610706972008\n",
      "  (654, 899)\t0.242442827888032\n",
      "  (654, 3888)\t0.051052488896697966\n",
      "  (654, 4221)\t0.2714650304890047\n",
      "  (654, 650)\t0.2580347664411713\n",
      "  (654, 2686)\t0.058502612126048806\n",
      "  (654, 1118)\t0.07679474858947376\n",
      "  (654, 4019)\t0.18357157584020387\n",
      "  (654, 3500)\t0.0698988013530705\n",
      "  (654, 2486)\t0.09072603158190014\n",
      "  (654, 1754)\t0.06694957982219636\n",
      "  (654, 4204)\t0.15358949717894751\n",
      "  (654, 3650)\t0.09072603158190014\n",
      "  (0, 3835)\t0.06410079410983066\n",
      "  (0, 3127)\t0.05506979793696032\n",
      "  (0, 1234)\t0.14148728361531826\n",
      "  (0, 2320)\t0.35125220511909655\n",
      "  (0, 2025)\t0.2310024290789113\n",
      "  (0, 3380)\t0.4237295114125377\n",
      "  (0, 2052)\t0.15395801965547096\n",
      "  (0, 961)\t0.25423770684752256\n",
      "  (0, 2322)\t0.33272649654294295\n",
      "  (0, 1628)\t0.3224038704618687\n",
      "  (0, 433)\t0.1297471048665307\n",
      "  (0, 473)\t0.01717152469532511\n",
      "  (0, 570)\t0.014613258757647558\n",
      "  (0, 1030)\t0.057560061635212295\n",
      "  (0, 1794)\t0.05101932916059392\n",
      "  (0, 1521)\t0.03514978048749194\n",
      "  (0, 660)\t0.03559921220338679\n",
      "  (0, 375)\t0.06410079410983066\n",
      "  (0, 4636)\t0.045738636603348766\n",
      "  (0, 4486)\t0.07697830041873661\n",
      "  (0, 2741)\t0.04643794034963594\n",
      "  (0, 3787)\t0.029775504198206976\n",
      "  (0, 4645)\t0.01669403924897024\n",
      "  (0, 4418)\t0.04285696914744371\n",
      "  (0, 3399)\t0.03559921220338679\n",
      "  :\t:\n",
      "  (654, 3978)\t0.060610706972008\n",
      "  (654, 2974)\t0.121221413944016\n",
      "  (654, 1379)\t0.055431470737939546\n",
      "  (654, 1535)\t0.05393497462982479\n",
      "  (654, 1159)\t0.26110655802086774\n",
      "  (654, 2370)\t0.04500648366260221\n",
      "  (654, 4067)\t0.05802108885497378\n",
      "  (654, 715)\t0.058502612126048806\n",
      "  (654, 209)\t0.055831597934352994\n",
      "  (654, 1078)\t0.041648917650643866\n",
      "  (654, 791)\t0.2580347664411713\n",
      "  (654, 3322)\t0.055831597934352994\n",
      "  (654, 2277)\t0.060610706972008\n",
      "  (654, 899)\t0.242442827888032\n",
      "  (654, 3888)\t0.051052488896697966\n",
      "  (654, 4221)\t0.2714650304890047\n",
      "  (654, 650)\t0.2580347664411713\n",
      "  (654, 2686)\t0.058502612126048806\n",
      "  (654, 1118)\t0.07679474858947376\n",
      "  (654, 4019)\t0.18357157584020387\n",
      "  (654, 3500)\t0.0698988013530705\n",
      "  (654, 2486)\t0.09072603158190014\n",
      "  (654, 1754)\t0.06694957982219636\n",
      "  (654, 4204)\t0.15358949717894751\n",
      "  (654, 3650)\t0.09072603158190014\n"
     ]
    }
   ],
   "source": [
    "import Sastrawi\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "\n",
    "# Test stemming dengan satu dokumen\n",
    "input_stemm = str(pta_df[\"abstrak_id_filtered\"].iloc[0])\n",
    "hasil_stemm = stemmer.stem(input_stemm)\n",
    "print(hasil_stemm)\n",
    "\n",
    "# Stemming untuk semua dokumen\n",
    "hasil_stemm = []\n",
    "for doc in pta_df[\"abstrak_id_filtered\"]:\n",
    "    stemmed_doc = [stemmer.stem(word) for word in doc]\n",
    "    hasil_stemm.append(stemmed_doc)\n",
    "\n",
    "# Konversi hasil stemming ke string untuk TF-IDF\n",
    "stemmed_texts = [' '.join(doc) for doc in hasil_stemm]\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "x = vectorizer.fit_transform(stemmed_texts)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File disimpan sebagai Management_dataHasilPreprocessing.csv\n"
     ]
    }
   ],
   "source": [
    "from nltk import FreqDist\n",
    "import pandas as pd\n",
    "\n",
    "# Gabungkan semua kata dari hasil stemming\n",
    "all_words = []\n",
    "for doc in hasil_stemm:\n",
    "    all_words.extend(doc)\n",
    "\n",
    "fdist = FreqDist(all_words)\n",
    "\n",
    "frequency_df = pd.DataFrame(\n",
    "    fdist.most_common(),\n",
    "    columns=['Word', 'Frequency']\n",
    ")\n",
    "\n",
    "frequency_df.to_csv('Management_dataHasilPreprocessing.csv', index=False, encoding='utf-8-sig')\n",
    "print(\"File disimpan sebagai Management_dataHasilPreprocessing.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5. Word Embeddings dengan CBOW (Continuous Bag of Words)**\n",
    "\n",
    "**CBOW** adalah salah satu arsitektur Word2Vec yang memprediksi kata target berdasarkan konteks di sekitarnya. Berbeda dengan TF-IDF yang menghasilkan sparse vectors, CBOW menghasilkan dense vectors yang dapat menangkap hubungan semantik antar kata. Model ini berguna untuk analisis similarity dan clustering dokumen berdasarkan makna kontekstual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Installing compatible packages...\n",
      "   Installing numpy==1.24.4...\n",
      "   ✅ numpy==1.24.4 installed successfully\n",
      "   Installing scipy==1.10.1...\n",
      "   ✅ numpy==1.24.4 installed successfully\n",
      "   Installing scipy==1.10.1...\n",
      "   ✅ scipy==1.10.1 installed successfully\n",
      "   Installing gensim==4.3.0...\n",
      "   ✅ scipy==1.10.1 installed successfully\n",
      "   Installing gensim==4.3.0...\n",
      "   ✅ gensim==4.3.0 installed successfully\n",
      "\n",
      "🔄 Importing libraries...\n",
      "   ✅ numpy imported\n",
      "   ✅ pandas imported\n",
      "   ✅ sklearn imported\n",
      "   ✅ gensim Word2Vec imported\n",
      "🎉 Semua libraries berhasil diimport!\n",
      "=== ANALISIS WORD EMBEDDINGS DENGAN CBOW ===\n",
      "\n",
      "📊 Data untuk training CBOW:\n",
      "   • Total dokumen: 655\n",
      "   • Rata-rata panjang dokumen: 145.5 kata\n",
      "   • Total unique words: 4872\n",
      "\n",
      "🤖 Training Word2Vec model dengan CBOW...\n",
      "   ✅ gensim==4.3.0 installed successfully\n",
      "\n",
      "🔄 Importing libraries...\n",
      "   ✅ numpy imported\n",
      "   ✅ pandas imported\n",
      "   ✅ sklearn imported\n",
      "   ✅ gensim Word2Vec imported\n",
      "🎉 Semua libraries berhasil diimport!\n",
      "=== ANALISIS WORD EMBEDDINGS DENGAN CBOW ===\n",
      "\n",
      "📊 Data untuk training CBOW:\n",
      "   • Total dokumen: 655\n",
      "   • Rata-rata panjang dokumen: 145.5 kata\n",
      "   • Total unique words: 4872\n",
      "\n",
      "🤖 Training Word2Vec model dengan CBOW...\n",
      "✅ Model CBOW berhasil dilatih!\n",
      "   • Vocabulary size: 2818 kata\n",
      "   • Vector dimensions: 100\n",
      "   • Training epochs: 100\n",
      "\n",
      "🔍 ANALISIS SEMANTIC SIMILARITY:\n",
      "\n",
      "📝 Kata mirip dengan 'manajemen':\n",
      "      bersih          | Similarity: 0.4004\n",
      "      hrsc            | Similarity: 0.3846\n",
      "      utm             | Similarity: 0.3826\n",
      "      bakat           | Similarity: 0.3597\n",
      "      susun           | Similarity: 0.3271\n",
      "\n",
      "📝 Kata mirip dengan 'perusahaanperusahaan':\n",
      "      tawar           | Similarity: 0.5036\n",
      "      sumbu           | Similarity: 0.5002\n",
      "      usah            | Similarity: 0.4990\n",
      "      ketat           | Similarity: 0.4988\n",
      "      bawa            | Similarity: 0.4685\n",
      "\n",
      "📝 Kata mirip dengan 'kualitaskinerja':\n",
      "      hargavalue      | Similarity: 0.7456\n",
      "      fungsional      | Similarity: 0.7179\n",
      "      emosional       | Similarity: 0.5555\n",
      "      money           | Similarity: 0.5520\n",
      "      customer        | Similarity: 0.5359\n",
      "\n",
      "📝 Kata mirip dengan 'strategi':\n",
      "      relatif         | Similarity: 0.5747\n",
      "      terap           | Similarity: 0.5548\n",
      "      saing           | Similarity: 0.5305\n",
      "      luas            | Similarity: 0.5191\n",
      "      unggul          | Similarity: 0.5065\n",
      "\n",
      "📝 Kata mirip dengan 'analisis':\n",
      "      uji             | Similarity: 0.6569\n",
      "      analisa         | Similarity: 0.5739\n",
      "      regresi         | Similarity: 0.5685\n",
      "      ganda           | Similarity: 0.5435\n",
      "      olah            | Similarity: 0.5325\n",
      "\n",
      "============================================================\n",
      "✅ Model CBOW berhasil dilatih!\n",
      "   • Vocabulary size: 2818 kata\n",
      "   • Vector dimensions: 100\n",
      "   • Training epochs: 100\n",
      "\n",
      "🔍 ANALISIS SEMANTIC SIMILARITY:\n",
      "\n",
      "📝 Kata mirip dengan 'manajemen':\n",
      "      bersih          | Similarity: 0.4004\n",
      "      hrsc            | Similarity: 0.3846\n",
      "      utm             | Similarity: 0.3826\n",
      "      bakat           | Similarity: 0.3597\n",
      "      susun           | Similarity: 0.3271\n",
      "\n",
      "📝 Kata mirip dengan 'perusahaanperusahaan':\n",
      "      tawar           | Similarity: 0.5036\n",
      "      sumbu           | Similarity: 0.5002\n",
      "      usah            | Similarity: 0.4990\n",
      "      ketat           | Similarity: 0.4988\n",
      "      bawa            | Similarity: 0.4685\n",
      "\n",
      "📝 Kata mirip dengan 'kualitaskinerja':\n",
      "      hargavalue      | Similarity: 0.7456\n",
      "      fungsional      | Similarity: 0.7179\n",
      "      emosional       | Similarity: 0.5555\n",
      "      money           | Similarity: 0.5520\n",
      "      customer        | Similarity: 0.5359\n",
      "\n",
      "📝 Kata mirip dengan 'strategi':\n",
      "      relatif         | Similarity: 0.5747\n",
      "      terap           | Similarity: 0.5548\n",
      "      saing           | Similarity: 0.5305\n",
      "      luas            | Similarity: 0.5191\n",
      "      unggul          | Similarity: 0.5065\n",
      "\n",
      "📝 Kata mirip dengan 'analisis':\n",
      "      uji             | Similarity: 0.6569\n",
      "      analisa         | Similarity: 0.5739\n",
      "      regresi         | Similarity: 0.5685\n",
      "      ganda           | Similarity: 0.5435\n",
      "      olah            | Similarity: 0.5325\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Install dan konfigurasi libraries untuk CBOW dengan versi kompatibel\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "def install_package(package):\n",
    "    \"\"\"Install package dengan handling error\"\"\"\n",
    "    try:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "        return True\n",
    "    except subprocess.CalledProcessError:\n",
    "        return False\n",
    "\n",
    "# Install packages dengan versi yang kompatibel\n",
    "print(\"📦 Installing compatible packages...\")\n",
    "\n",
    "packages = [\n",
    "    \"numpy==1.24.4\",  # Versi yang lebih kompatibel\n",
    "    \"scipy==1.10.1\",   # Versi yang lebih stabil\n",
    "    \"gensim==4.3.0\"    # Versi gensim yang stabil\n",
    "]\n",
    "\n",
    "for package in packages:\n",
    "    print(f\"   Installing {package}...\")\n",
    "    if install_package(package):\n",
    "        print(f\"   ✅ {package} installed successfully\")\n",
    "    else:\n",
    "        print(f\"   ⚠️ Failed to install {package}, trying alternative...\")\n",
    "\n",
    "# Import libraries dengan error handling yang lebih baik\n",
    "print(\"\\n🔄 Importing libraries...\")\n",
    "try:\n",
    "    # Force reload modules jika sudah ada\n",
    "    import importlib\n",
    "    \n",
    "    # Import dengan try-except individual\n",
    "    import numpy as np\n",
    "    print(\"   ✅ numpy imported\")\n",
    "    \n",
    "    import pandas as pd\n",
    "    print(\"   ✅ pandas imported\")\n",
    "    \n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    print(\"   ✅ sklearn imported\")\n",
    "    \n",
    "    # Import gensim dengan handling khusus\n",
    "    try:\n",
    "        from gensim.models import Word2Vec\n",
    "        print(\"   ✅ gensim Word2Vec imported\")\n",
    "        gensim_available = True\n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ gensim import error: {e}\")\n",
    "        print(\"   💡 Trying alternative import method...\")\n",
    "        \n",
    "        try:\n",
    "            # Alternative import method\n",
    "            import gensim\n",
    "            from gensim import models\n",
    "            Word2Vec = models.Word2Vec\n",
    "            print(\"   ✅ gensim imported via alternative method\")\n",
    "            gensim_available = True\n",
    "        except Exception as e2:\n",
    "            print(f\"   ❌ Alternative gensim import failed: {e2}\")\n",
    "            gensim_available = False\n",
    "            \n",
    "    if gensim_available:\n",
    "        print(\"🎉 Semua libraries berhasil diimport!\")\n",
    "    else:\n",
    "        print(\"⚠️ Gensim tidak tersedia, akan skip analisis CBOW\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Critical import error: {e}\")\n",
    "    print(\"💡 Silakan restart kernel dan coba lagi\")\n",
    "\n",
    "# Lanjutkan jika gensim berhasil diimport\n",
    "if 'gensim_available' in locals() and gensim_available:\n",
    "    print(\"=== ANALISIS WORD EMBEDDINGS DENGAN CBOW ===\\n\")\n",
    "\n",
    "    # Siapkan data untuk Word2Vec (perlu dalam format list of sentences)\n",
    "    sentences = hasil_stemm  # hasil_stemm sudah berupa list of lists (sentences)\n",
    "\n",
    "    print(f\"📊 Data untuk training CBOW:\")\n",
    "    print(f\"   • Total dokumen: {len(sentences)}\")\n",
    "    print(f\"   • Rata-rata panjang dokumen: {np.mean([len(sent) for sent in sentences]):.1f} kata\")\n",
    "    print(f\"   • Total unique words: {len(set([word for sent in sentences for word in sent]))}\")\n",
    "\n",
    "    # Training Word2Vec dengan arsitektur CBOW\n",
    "    print(f\"\\n🤖 Training Word2Vec model dengan CBOW...\")\n",
    "    try:\n",
    "        cbow_model = Word2Vec(\n",
    "            sentences=sentences,\n",
    "            vector_size=100,        # Dimensi embedding (dense vector)\n",
    "            window=5,              # Ukuran context window\n",
    "            min_count=2,           # Minimum frekuensi kata untuk dimasukkan\n",
    "            sg=0,                  # sg=0 untuk CBOW, sg=1 untuk Skip-gram\n",
    "            workers=4,             # Jumlah thread untuk training\n",
    "            epochs=100,            # Jumlah iterasi training\n",
    "            seed=42                # Untuk reproducibility\n",
    "        )\n",
    "\n",
    "        print(f\"✅ Model CBOW berhasil dilatih!\")\n",
    "        print(f\"   • Vocabulary size: {len(cbow_model.wv)} kata\")\n",
    "        print(f\"   • Vector dimensions: {cbow_model.wv.vector_size}\")\n",
    "        print(f\"   • Training epochs: {cbow_model.epochs}\")\n",
    "\n",
    "        # Analisis kata-kata terdekat\n",
    "        print(f\"\\n🔍 ANALISIS SEMANTIC SIMILARITY:\")\n",
    "\n",
    "        # Pilih beberapa kata kunci untuk analisis\n",
    "        sample_words = []\n",
    "        vocab_words = list(cbow_model.wv.key_to_index.keys())\n",
    "\n",
    "        # Ambil kata-kata yang relevan dengan domain manajemen\n",
    "        target_keywords = ['manaj', 'perusaha', 'kinerja', 'strategi', 'analisis', 'peneliti', 'hasil', 'faktor']\n",
    "        for keyword in target_keywords:\n",
    "            # Cari kata yang mengandung keyword\n",
    "            matching_words = [word for word in vocab_words if keyword in word]\n",
    "            if matching_words:\n",
    "                sample_words.append(matching_words[0])  # Ambil yang pertama\n",
    "\n",
    "        # Tampilkan analisis similarity untuk beberapa kata\n",
    "        for word in sample_words[:5]:  # Ambil 5 kata pertama\n",
    "            if word in cbow_model.wv:\n",
    "                try:\n",
    "                    similar_words = cbow_model.wv.most_similar(word, topn=5)\n",
    "                    print(f\"\\n📝 Kata mirip dengan '{word}':\")\n",
    "                    for similar_word, score in similar_words:\n",
    "                        print(f\"      {similar_word:15s} | Similarity: {score:.4f}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"   Tidak bisa mencari kata mirip untuk '{word}': {e}\")\n",
    "\n",
    "        cbow_success = True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error training CBOW model: {e}\")\n",
    "        print(\"💡 Model tidak dapat dilatih, akan skip analisis lanjutan\")\n",
    "        cbow_success = False\n",
    "\n",
    "    print(f\"\\n\" + \"=\"*60)\n",
    "\n",
    "else:\n",
    "    print(\"⚠️ GENSIM TIDAK TERSEDIA - SKIP ANALISIS CBOW\")\n",
    "    print(\"Analisis CBOW memerlukan library gensim yang tidak dapat diimport.\")\n",
    "    print(\"Silakan install ulang atau gunakan environment yang berbeda.\")\n",
    "    cbow_success = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔢 PEMBUATAN DOCUMENT EMBEDDINGS\n",
      "==================================================\n",
      "\n",
      "📋 Membuat embeddings untuk 655 dokumen...\n",
      "✅ Document embeddings berhasil dibuat!\n",
      "   • Shape: (655, 100)\n",
      "   • Rata-rata kata valid per dokumen: 142.4\n",
      "   • Coverage rata-rata: 0.971\n",
      "\n",
      "🔗 Menghitung similarity antar dokumen...\n",
      "✅ Similarity matrix berhasil dibuat!\n",
      "   • Shape: (655, 655)\n",
      "   • Rata-rata similarity: 0.2804\n",
      "\n",
      "🎯 ANALISIS DOKUMEN PALING MIRIP:\n",
      "=============================================\n",
      "\n",
      "📊 Top 5 pasangan dokumen paling mirip:\n",
      "\n",
      "1. Dokumen 253 ↔ Dokumen 303\n",
      "   📈 Similarity Score: 1.0000\n",
      "   📝 Preview Doc 253: maulina hotim pengaruh pribadi mampu individu prestasi kerja karyawan dinas...\n",
      "   📝 Preview Doc 303: maulina hotim pengaruh pribadi mampu individu prestasi kerja karyawan dinas...\n",
      "\n",
      "2. Dokumen 331 ↔ Dokumen 647\n",
      "   📈 Similarity Score: 0.9871\n",
      "   📝 Preview Doc 331: teliti pt bank rakyat indonesia persero tbk cabang diponegoro surabaya...\n",
      "   📝 Preview Doc 647: teliti judul pengaruh rotasi kerja motivasi kerja kerja karyawan puas...\n",
      "\n",
      "3. Dokumen 376 ↔ Dokumen 483\n",
      "   📈 Similarity Score: 0.9860\n",
      "   📝 Preview Doc 376: tuju teliti pengaruh struktur modal long term debt to equity...\n",
      "   📝 Preview Doc 483: tuju teliti pengaruh struktur modal long term debt to equity...\n",
      "\n",
      "4. Dokumen 104 ↔ Dokumen 427\n",
      "   📈 Similarity Score: 0.9851\n",
      "   📝 Preview Doc 104: teliti tuju hitung rencana laba inap wisma koperasi analisa cost...\n",
      "   📝 Preview Doc 427: teliti tuju hitung rencana laba hotel ningrat bangkal analisa cost...\n",
      "\n",
      "5. Dokumen 230 ↔ Dokumen 349\n",
      "   📈 Similarity Score: 0.9832\n",
      "   📝 Preview Doc 230: tuju teliti ukur kerja uang bank bumn bni btn bri...\n",
      "   📝 Preview Doc 349: dewi kartini nilai kerja uang rasio capital asset earnings liquidity...\n",
      "\n",
      "============================================================\n",
      "✅ Document embeddings berhasil dibuat!\n",
      "   • Shape: (655, 100)\n",
      "   • Rata-rata kata valid per dokumen: 142.4\n",
      "   • Coverage rata-rata: 0.971\n",
      "\n",
      "🔗 Menghitung similarity antar dokumen...\n",
      "✅ Similarity matrix berhasil dibuat!\n",
      "   • Shape: (655, 655)\n",
      "   • Rata-rata similarity: 0.2804\n",
      "\n",
      "🎯 ANALISIS DOKUMEN PALING MIRIP:\n",
      "=============================================\n",
      "\n",
      "📊 Top 5 pasangan dokumen paling mirip:\n",
      "\n",
      "1. Dokumen 253 ↔ Dokumen 303\n",
      "   📈 Similarity Score: 1.0000\n",
      "   📝 Preview Doc 253: maulina hotim pengaruh pribadi mampu individu prestasi kerja karyawan dinas...\n",
      "   📝 Preview Doc 303: maulina hotim pengaruh pribadi mampu individu prestasi kerja karyawan dinas...\n",
      "\n",
      "2. Dokumen 331 ↔ Dokumen 647\n",
      "   📈 Similarity Score: 0.9871\n",
      "   📝 Preview Doc 331: teliti pt bank rakyat indonesia persero tbk cabang diponegoro surabaya...\n",
      "   📝 Preview Doc 647: teliti judul pengaruh rotasi kerja motivasi kerja kerja karyawan puas...\n",
      "\n",
      "3. Dokumen 376 ↔ Dokumen 483\n",
      "   📈 Similarity Score: 0.9860\n",
      "   📝 Preview Doc 376: tuju teliti pengaruh struktur modal long term debt to equity...\n",
      "   📝 Preview Doc 483: tuju teliti pengaruh struktur modal long term debt to equity...\n",
      "\n",
      "4. Dokumen 104 ↔ Dokumen 427\n",
      "   📈 Similarity Score: 0.9851\n",
      "   📝 Preview Doc 104: teliti tuju hitung rencana laba inap wisma koperasi analisa cost...\n",
      "   📝 Preview Doc 427: teliti tuju hitung rencana laba hotel ningrat bangkal analisa cost...\n",
      "\n",
      "5. Dokumen 230 ↔ Dokumen 349\n",
      "   📈 Similarity Score: 0.9832\n",
      "   📝 Preview Doc 230: tuju teliti ukur kerja uang bank bumn bni btn bri...\n",
      "   📝 Preview Doc 349: dewi kartini nilai kerja uang rasio capital asset earnings liquidity...\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Lanjutkan document embeddings jika CBOW berhasil\n",
    "if 'cbow_success' in locals() and cbow_success and 'cbow_model' in locals():\n",
    "    # Membuat Document Embeddings dengan rata-rata Word Vectors\n",
    "    print(f\"🔢 PEMBUATAN DOCUMENT EMBEDDINGS\")\n",
    "    print(f\"=\"*50)\n",
    "\n",
    "    def create_document_embedding(tokens, model):\n",
    "        \"\"\"Membuat embedding dokumen dengan rata-rata word vectors\"\"\"\n",
    "        vectors = []\n",
    "        valid_words = 0\n",
    "        \n",
    "        for word in tokens:\n",
    "            if word in model.wv:\n",
    "                vectors.append(model.wv[word])\n",
    "                valid_words += 1\n",
    "        \n",
    "        if vectors:\n",
    "            # Rata-rata dari semua word vectors dalam dokumen\n",
    "            doc_embedding = np.mean(vectors, axis=0)\n",
    "            return doc_embedding, valid_words\n",
    "        else:\n",
    "            # Jika tidak ada kata yang valid, return zero vector\n",
    "            return np.zeros(model.wv.vector_size), 0\n",
    "else:\n",
    "    print(\"⚠️ SKIP DOCUMENT EMBEDDINGS - CBOW MODEL TIDAK TERSEDIA\")\n",
    "    print(\"Document embeddings memerlukan model CBOW yang sudah dilatih.\")\n",
    "    \n",
    "if 'cbow_success' in locals() and cbow_success and 'cbow_model' in locals():\n",
    "\n",
    "    # Buat embeddings untuk semua dokumen\n",
    "    print(f\"\\n📋 Membuat embeddings untuk {len(sentences)} dokumen...\")\n",
    "\n",
    "    document_embeddings = []\n",
    "    valid_words_count = []\n",
    "    doc_info = []\n",
    "\n",
    "    for idx, tokens in enumerate(sentences):\n",
    "        embedding, valid_count = create_document_embedding(tokens, cbow_model)\n",
    "        document_embeddings.append(embedding)\n",
    "        valid_words_count.append(valid_count)\n",
    "        \n",
    "        # Simpan informasi dokumen\n",
    "        original_text = ' '.join(tokens[:10])  # 10 kata pertama sebagai preview\n",
    "        doc_info.append({\n",
    "            'doc_id': idx,\n",
    "            'preview': original_text + ('...' if len(tokens) > 10 else ''),\n",
    "            'total_words': len(tokens),\n",
    "            'valid_words': valid_count,\n",
    "            'coverage': valid_count / len(tokens) if len(tokens) > 0 else 0\n",
    "        })\n",
    "\n",
    "    # Convert ke numpy array\n",
    "    document_embeddings = np.array(document_embeddings)\n",
    "\n",
    "    print(f\"✅ Document embeddings berhasil dibuat!\")\n",
    "    print(f\"   • Shape: {document_embeddings.shape}\")\n",
    "    print(f\"   • Rata-rata kata valid per dokumen: {np.mean(valid_words_count):.1f}\")\n",
    "    print(f\"   • Coverage rata-rata: {np.mean([info['coverage'] for info in doc_info]):.3f}\")\n",
    "\n",
    "    # Hitung Document Similarity Matrix menggunakan Cosine Similarity\n",
    "    print(f\"\\n🔗 Menghitung similarity antar dokumen...\")\n",
    "    doc_similarity_matrix = cosine_similarity(document_embeddings)\n",
    "\n",
    "    print(f\"✅ Similarity matrix berhasil dibuat!\")\n",
    "    print(f\"   • Shape: {doc_similarity_matrix.shape}\")\n",
    "    print(f\"   • Rata-rata similarity: {np.mean(doc_similarity_matrix[np.triu_indices_from(doc_similarity_matrix, k=1)]):.4f}\")\n",
    "\n",
    "    # Analisis dokumen paling mirip\n",
    "    print(f\"\\n🎯 ANALISIS DOKUMEN PALING MIRIP:\")\n",
    "    print(f\"=\"*45)\n",
    "\n",
    "    # Cari pasangan dokumen dengan similarity tertinggi\n",
    "    upper_triangle = np.triu_indices_from(doc_similarity_matrix, k=1)\n",
    "    similarities = doc_similarity_matrix[upper_triangle]\n",
    "    sorted_indices = np.argsort(similarities)[::-1]\n",
    "\n",
    "    print(f\"\\n📊 Top 5 pasangan dokumen paling mirip:\")\n",
    "    for i in range(min(5, len(sorted_indices))):\n",
    "        idx = sorted_indices[i]\n",
    "        doc_i, doc_j = upper_triangle[0][idx], upper_triangle[1][idx]\n",
    "        similarity_score = similarities[idx]\n",
    "        \n",
    "        print(f\"\\n{i+1}. Dokumen {doc_i} ↔ Dokumen {doc_j}\")\n",
    "        print(f\"   📈 Similarity Score: {similarity_score:.4f}\")\n",
    "        print(f\"   📝 Preview Doc {doc_i}: {doc_info[doc_i]['preview']}\")\n",
    "        print(f\"   📝 Preview Doc {doc_j}: {doc_info[doc_j]['preview']}\")\n",
    "\n",
    "    print(f\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 EXPORT HASIL CBOW KE EXCEL MULTI-SHEET\n",
      "==================================================\n",
      "✅ Excel CBOW berhasil dibuat dengan 5 sheet komprehensif:\n",
      "   📊 CBOW_Analysis_Formatted.xlsx - Analisis lengkap CBOW\n",
      "      • Sheet 1: Penjelasan_CBOW - Konsep dan teori CBOW vs TF-IDF\n",
      "      • Sheet 2: Detail_Training - Sample word vectors dan similarity\n",
      "      • Sheet 3: Analisis_Similarity - Top pasangan dokumen mirip\n",
      "      • Sheet 4: Ringkasan_Embeddings - Summary document embeddings\n",
      "      • Sheet 5: Statistik_Model - Comprehensive model statistics\n",
      "\n",
      "📈 CBOW Analysis Summary:\n",
      "   • Total documents analyzed: 655\n",
      "   • Vocabulary size: 2,818\n",
      "   • Vector dimensions: 100\n",
      "   • Average document similarity: 0.280438\n",
      "   • Top document similarity: 1.000000\n",
      "\n",
      "============================================================\n",
      "✅ Excel CBOW berhasil dibuat dengan 5 sheet komprehensif:\n",
      "   📊 CBOW_Analysis_Formatted.xlsx - Analisis lengkap CBOW\n",
      "      • Sheet 1: Penjelasan_CBOW - Konsep dan teori CBOW vs TF-IDF\n",
      "      • Sheet 2: Detail_Training - Sample word vectors dan similarity\n",
      "      • Sheet 3: Analisis_Similarity - Top pasangan dokumen mirip\n",
      "      • Sheet 4: Ringkasan_Embeddings - Summary document embeddings\n",
      "      • Sheet 5: Statistik_Model - Comprehensive model statistics\n",
      "\n",
      "📈 CBOW Analysis Summary:\n",
      "   • Total documents analyzed: 655\n",
      "   • Vocabulary size: 2,818\n",
      "   • Vector dimensions: 100\n",
      "   • Average document similarity: 0.280438\n",
      "   • Top document similarity: 1.000000\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# EXPORT HASIL CBOW KE EXCEL DENGAN FORMAT PROFESIONAL\n",
    "if 'cbow_success' in locals() and cbow_success and 'document_embeddings' in locals():\n",
    "    print(f\"💾 EXPORT HASIL CBOW KE EXCEL MULTI-SHEET\")\n",
    "    print(f\"=\"*50)\n",
    "    \n",
    "    try:\n",
    "        from openpyxl import Workbook\n",
    "        from openpyxl.styles import Font, PatternFill, Border, Side, Alignment\n",
    "        from openpyxl.utils.dataframe import dataframe_to_rows\n",
    "        from openpyxl.formatting.rule import ColorScaleRule\n",
    "        \n",
    "        # Buat workbook baru untuk CBOW\n",
    "        wb_cbow = Workbook()\n",
    "        \n",
    "        # Definisi styles yang sama seperti TF-IDF\n",
    "        header_font = Font(bold=True, color=\"FFFFFF\", size=11)\n",
    "        header_fill = PatternFill(start_color=\"2E8B57\", end_color=\"2E8B57\", fill_type=\"solid\")  # Sea Green untuk CBOW\n",
    "        header_alignment = Alignment(horizontal=\"center\", vertical=\"center\", wrap_text=True)\n",
    "        \n",
    "        data_font = Font(size=9)\n",
    "        data_alignment_left = Alignment(horizontal=\"left\", vertical=\"center\")\n",
    "        data_alignment_center = Alignment(horizontal=\"center\", vertical=\"center\")\n",
    "        data_alignment_right = Alignment(horizontal=\"right\", vertical=\"center\")\n",
    "        \n",
    "        thin_border = Border(\n",
    "            left=Side(style='thin'), right=Side(style='thin'),\n",
    "            top=Side(style='thin'), bottom=Side(style='thin')\n",
    "        )\n",
    "        \n",
    "        # Sheet 1: Penjelasan Konsep CBOW\n",
    "        ws1 = wb_cbow.active\n",
    "        ws1.title = \"Penjelasan_CBOW\"\n",
    "        \n",
    "        penjelasan_cbow = [\n",
    "            ['Aspek', 'CBOW (Continuous Bag of Words)', 'Penjelasan Detail', 'Contoh Aplikasi'],\n",
    "            ['Definisi', 'Neural network yang memprediksi kata target dari konteks', 'Menggunakan kata-kata di sekitar untuk memprediksi kata tengah', 'Prediksi kata hilang dalam kalimat'],\n",
    "            ['Arsitektur', 'Input: context words → Hidden layer → Output: target word', 'Rata-rata word vectors sebagai input ke hidden layer', 'Context: [penelitian, ini, tentang] → Target: menganalisis'],\n",
    "            ['Vector Output', 'Dense vectors (100 dimensi)', 'Representasi padat yang menangkap semantic similarity', 'Kata \"analisis\" dan \"penelitian\" memiliki vector yang mirip'],\n",
    "            ['Training Process', 'Sliding window melewati corpus', 'Window bergeser untuk create training pairs', 'Window size=5: ambil 2 kata kiri-kanan sebagai context'],\n",
    "            ['', '', '', ''],\n",
    "            ['Perbedaan dengan TF-IDF', '', '', ''],\n",
    "            ['Representasi', 'Dense vectors (padat)', 'TF-IDF: Sparse vectors (jarang)', 'CBOW: 100 dimensi, TF-IDF: 6475 dimensi'],\n",
    "            ['Basis Perhitungan', 'Semantic context', 'TF-IDF: Statistical frequency', 'CBOW: makna, TF-IDF: frekuensi'],\n",
    "            ['Similarity Measure', 'Cosine similarity antar dense vectors', 'TF-IDF: Dot product atau cosine', 'CBOW menangkap hubungan semantic lebih baik'],\n",
    "            ['Interpretasi', 'Semantic relationships', 'TF-IDF: Term importance', 'CBOW: kata mirip secara makna'],\n",
    "            ['', '', '', ''],\n",
    "            ['Parameter Model CBOW', '', '', ''],\n",
    "            [f'Vector Size', f'{cbow_model.wv.vector_size}', 'Dimensi embedding untuk setiap kata', 'Lebih tinggi = representasi lebih detail'],\n",
    "            [f'Window Size', f'{cbow_model.window}', 'Jumlah kata konteks kiri-kanan', 'Window=5 berarti 5 kata sebelum + 5 setelah'],\n",
    "            [f'Min Count', f'{cbow_model.min_count}', 'Frekuensi minimum kata untuk dimasukkan', 'Kata < 2x diabaikan untuk mengurangi noise'],\n",
    "            [f'Training Epochs', f'{cbow_model.epochs}', 'Jumlah iterasi training', 'Lebih banyak epoch = learning lebih dalam'],\n",
    "            [f'Total Vocabulary', f'{len(cbow_model.wv)}', 'Ukuran kosakata model', 'Kata-kata yang dipelajari oleh model'],\n",
    "            ['', '', '', ''],\n",
    "            ['Statistik Dataset', '', '', ''],\n",
    "            [f'Total Dokumen', f'{len(sentences)}', 'Jumlah abstrak yang dianalisis', 'Corpus penelitian manajemen'],\n",
    "            [f'Rata-rata Panjang Dokumen', f'{np.mean([len(sent) for sent in sentences]):.1f} kata', 'Panjang dokumen setelah preprocessing', 'Termasuk stemming dan stopword removal'],\n",
    "            [f'Coverage Rata-rata', f'{np.mean([info[\"coverage\"] for info in doc_info]):.3f}', 'Persentase kata yang dikenali model', 'Kata yang ada di vocabulary model'],\n",
    "            [f'Similarity Rata-rata', f'{np.mean(doc_similarity_matrix[np.triu_indices_from(doc_similarity_matrix, k=1)]):.4f}', 'Rata-rata kesamaan antar dokumen', 'Berbasis semantic similarity'],\n",
    "            [f'Max Similarity', f'{np.max(doc_similarity_matrix[np.triu_indices_from(doc_similarity_matrix, k=1)]):.4f}', 'Similarity tertinggi antar dokumen', 'Dokumen paling mirip secara semantic']\n",
    "        ]\n",
    "        \n",
    "        for row in penjelasan_cbow:\n",
    "            ws1.append(row)\n",
    "        \n",
    "        # Format sheet penjelasan CBOW\n",
    "        for row_idx, row in enumerate(ws1.iter_rows(min_row=1, max_row=len(penjelasan_cbow)), 1):\n",
    "            for col_idx, cell in enumerate(row):\n",
    "                if row_idx == 1 or row_idx == 7 or row_idx == 13 or row_idx == 19:  # Headers\n",
    "                    cell.font = Font(bold=True, size=11, color=\"FFFFFF\")\n",
    "                    cell.fill = PatternFill(start_color=\"2E8B57\", end_color=\"2E8B57\", fill_type=\"solid\")\n",
    "                elif row_idx in [6, 12, 18]:  # Empty rows\n",
    "                    continue\n",
    "                else:\n",
    "                    cell.font = Font(size=10)\n",
    "                \n",
    "                cell.alignment = Alignment(horizontal=\"left\", vertical=\"top\", wrap_text=True)\n",
    "                cell.border = thin_border\n",
    "        \n",
    "        # Set column widths\n",
    "        ws1.column_dimensions['A'].width = 25\n",
    "        ws1.column_dimensions['B'].width = 35\n",
    "        ws1.column_dimensions['C'].width = 45\n",
    "        ws1.column_dimensions['D'].width = 35\n",
    "        \n",
    "        # Sheet 2: Training Details\n",
    "        ws2 = wb_cbow.create_sheet(\"Detail_Training\")\n",
    "        \n",
    "        # Buat data training details\n",
    "        training_details = []\n",
    "        \n",
    "        # Ambil sample dari vocabulary untuk menunjukkan word vectors\n",
    "        sample_vocab = list(cbow_model.wv.key_to_index.keys())[:30]  # Top 30 kata\n",
    "        \n",
    "        for word in sample_vocab:\n",
    "            word_vector = cbow_model.wv[word]\n",
    "            frequency = cbow_model.wv.get_vecattr(word, 'count')\n",
    "            vector_norm = np.linalg.norm(word_vector)\n",
    "            \n",
    "            # Cari kata-kata paling mirip\n",
    "            try:\n",
    "                similar_words = cbow_model.wv.most_similar(word, topn=3)\n",
    "                top_similar = ', '.join([f\"{w}({s:.3f})\" for w, s in similar_words])\n",
    "            except:\n",
    "                top_similar = \"N/A\"\n",
    "            \n",
    "            training_details.append({\n",
    "                'Kata': word,\n",
    "                'Frekuensi': frequency,\n",
    "                'Vector_Norm': round(vector_norm, 4),\n",
    "                'Kata_Mirip_Top3': top_similar,\n",
    "                'Dimensi_1': round(word_vector[0], 4),\n",
    "                'Dimensi_2': round(word_vector[1], 4),\n",
    "                'Dimensi_3': round(word_vector[2], 4),\n",
    "                'Dimensi_4': round(word_vector[3], 4),\n",
    "                'Dimensi_5': round(word_vector[4], 4)\n",
    "            })\n",
    "        \n",
    "        training_df = pd.DataFrame(training_details)\n",
    "        \n",
    "        # Tulis training data\n",
    "        for r in dataframe_to_rows(training_df, index=False, header=True):\n",
    "            ws2.append(r)\n",
    "        \n",
    "        # Format header sheet 2\n",
    "        for cell in ws2[1]:\n",
    "            cell.font = header_font\n",
    "            cell.fill = header_fill\n",
    "            cell.alignment = header_alignment\n",
    "            cell.border = thin_border\n",
    "        \n",
    "        # Set column widths dan alignment\n",
    "        ws2_widths = [15, 12, 15, 40, 12, 12, 12, 12, 12]\n",
    "        ws2_alignments = [data_alignment_left, data_alignment_center, data_alignment_right,\n",
    "                         data_alignment_left, data_alignment_right, data_alignment_right, \n",
    "                         data_alignment_right, data_alignment_right, data_alignment_right]\n",
    "        \n",
    "        for idx, (width, alignment) in enumerate(zip(ws2_widths, ws2_alignments)):\n",
    "            col_letter = chr(65 + idx)\n",
    "            ws2.column_dimensions[col_letter].width = width\n",
    "            \n",
    "            for row in ws2.iter_rows(min_row=2, min_col=idx+1, max_col=idx+1):\n",
    "                for cell in row:\n",
    "                    cell.font = data_font\n",
    "                    cell.alignment = alignment\n",
    "                    cell.border = thin_border\n",
    "                    if idx in [1]:  # Frequency\n",
    "                        cell.number_format = '0'\n",
    "                    elif idx in [2, 4, 5, 6, 7, 8]:  # Vector values\n",
    "                        cell.number_format = '0.0000'\n",
    "        \n",
    "        ws2.freeze_panes = \"A2\"\n",
    "        ws2.auto_filter.ref = ws2.dimensions\n",
    "        ws2.row_dimensions[1].height = 30\n",
    "        \n",
    "        # Sheet 3: Document Similarity Analysis\n",
    "        ws3 = wb_cbow.create_sheet(\"Analisis_Similarity\")\n",
    "        \n",
    "        # Buat data untuk top similar document pairs\n",
    "        similar_pairs = []\n",
    "        for i in range(min(50, len(sorted_indices))):\n",
    "            idx = sorted_indices[i]\n",
    "            doc_i, doc_j = upper_triangle[0][idx], upper_triangle[1][idx]\n",
    "            similarity_score = similarities[idx]\n",
    "            \n",
    "            similar_pairs.append({\n",
    "                'Ranking': i + 1,\n",
    "                'ID_Dokumen_1': doc_i,\n",
    "                'ID_Dokumen_2': doc_j,\n",
    "                'Skor_Similarity': round(similarity_score, 6),\n",
    "                'Preview_Dok_1': doc_info[doc_i]['preview'][:50] + \"...\",\n",
    "                'Preview_Dok_2': doc_info[doc_j]['preview'][:50] + \"...\",\n",
    "                'Panjang_Dok_1': doc_info[doc_i]['total_words'],\n",
    "                'Panjang_Dok_2': doc_info[doc_j]['total_words'],\n",
    "                'Coverage_1': round(doc_info[doc_i]['coverage'], 4),\n",
    "                'Coverage_2': round(doc_info[doc_j]['coverage'], 4),\n",
    "                'Kategori_Similarity': 'Sangat Tinggi' if similarity_score > 0.8 else 'Tinggi' if similarity_score > 0.6 else 'Sedang' if similarity_score > 0.4 else 'Rendah'\n",
    "            })\n",
    "        \n",
    "        similarity_pairs_df = pd.DataFrame(similar_pairs)\n",
    "        \n",
    "        # Tulis similarity data\n",
    "        for r in dataframe_to_rows(similarity_pairs_df, index=False, header=True):\n",
    "            ws3.append(r)\n",
    "        \n",
    "        # Format header sheet 3\n",
    "        for cell in ws3[1]:\n",
    "            cell.font = header_font\n",
    "            cell.fill = header_fill\n",
    "            cell.alignment = header_alignment\n",
    "            cell.border = thin_border\n",
    "        \n",
    "        # Set column widths\n",
    "        ws3_widths = [10, 15, 15, 15, 35, 35, 12, 12, 12, 12, 15]\n",
    "        ws3_alignments = [data_alignment_center, data_alignment_center, data_alignment_center,\n",
    "                         data_alignment_right, data_alignment_left, data_alignment_left,\n",
    "                         data_alignment_center, data_alignment_center, data_alignment_right, \n",
    "                         data_alignment_right, data_alignment_center]\n",
    "        \n",
    "        for idx, (width, alignment) in enumerate(zip(ws3_widths, ws3_alignments)):\n",
    "            col_letter = chr(65 + idx)\n",
    "            ws3.column_dimensions[col_letter].width = width\n",
    "            \n",
    "            for row in ws3.iter_rows(min_row=2, min_col=idx+1, max_col=idx+1):\n",
    "                for cell in row:\n",
    "                    cell.font = data_font\n",
    "                    cell.alignment = alignment\n",
    "                    cell.border = thin_border\n",
    "                    if idx in [3, 8, 9]:  # Similarity and coverage scores\n",
    "                        cell.number_format = '0.0000'\n",
    "                    elif idx in [6, 7]:  # Document lengths\n",
    "                        cell.number_format = '0'\n",
    "        \n",
    "        # Conditional formatting untuk similarity score\n",
    "        ws3.conditional_formatting.add(f\"D2:D{ws3.max_row}\", \n",
    "                                      ColorScaleRule(start_type='min', start_color='FFFFFF',\n",
    "                                                   mid_type='percentile', mid_value=50, mid_color='98FB98',\n",
    "                                                   end_type='max', end_color='228B22'))\n",
    "        \n",
    "        ws3.freeze_panes = \"A2\"\n",
    "        ws3.auto_filter.ref = ws3.dimensions\n",
    "        ws3.row_dimensions[1].height = 30\n",
    "        \n",
    "        # Sheet 4: Document Embeddings Summary  \n",
    "        ws4 = wb_cbow.create_sheet(\"Ringkasan_Embeddings\")\n",
    "        \n",
    "        # Buat summary data untuk dokumen embeddings\n",
    "        doc_summary = []\n",
    "        for idx, info in enumerate(doc_info[:100]):  # Top 100 dokumen\n",
    "            embedding = document_embeddings[idx]\n",
    "            \n",
    "            doc_summary.append({\n",
    "                'ID_Dokumen': info['doc_id'],\n",
    "                'Preview_Dokumen': info['preview'],\n",
    "                'Total_Kata': info['total_words'],\n",
    "                'Kata_Valid': info['valid_words'], \n",
    "                'Coverage': round(info['coverage'], 4),\n",
    "                'Embedding_Norm': round(np.linalg.norm(embedding), 4),\n",
    "                'Rata_Embedding': round(np.mean(embedding), 6),\n",
    "                'Std_Embedding': round(np.std(embedding), 6),\n",
    "                'Min_Embedding': round(np.min(embedding), 6),\n",
    "                'Max_Embedding': round(np.max(embedding), 6)\n",
    "            })\n",
    "        \n",
    "        doc_summary_df = pd.DataFrame(doc_summary)\n",
    "        \n",
    "        # Tulis document summary data\n",
    "        for r in dataframe_to_rows(doc_summary_df, index=False, header=True):\n",
    "            ws4.append(r)\n",
    "        \n",
    "        # Format header sheet 4\n",
    "        for cell in ws4[1]:\n",
    "            cell.font = header_font\n",
    "            cell.fill = header_fill\n",
    "            cell.alignment = header_alignment\n",
    "            cell.border = thin_border\n",
    "        \n",
    "        # Set column widths\n",
    "        ws4_widths = [12, 50, 12, 12, 12, 15, 15, 15, 15, 15]\n",
    "        ws4_alignments = [data_alignment_center, data_alignment_left, data_alignment_center,\n",
    "                         data_alignment_center, data_alignment_right, data_alignment_right,\n",
    "                         data_alignment_right, data_alignment_right, data_alignment_right, data_alignment_right]\n",
    "        \n",
    "        for idx, (width, alignment) in enumerate(zip(ws4_widths, ws4_alignments)):\n",
    "            col_letter = chr(65 + idx)\n",
    "            ws4.column_dimensions[col_letter].width = width\n",
    "            \n",
    "            for row in ws4.iter_rows(min_row=2, min_col=idx+1, max_col=idx+1):\n",
    "                for cell in row:\n",
    "                    cell.font = data_font\n",
    "                    cell.alignment = alignment\n",
    "                    cell.border = thin_border\n",
    "                    if idx in [2, 3]:  # Word counts\n",
    "                        cell.number_format = '0'\n",
    "                    elif idx in [4, 5, 6, 7, 8, 9]:  # Decimal values\n",
    "                        if idx == 4:  # Coverage\n",
    "                            cell.number_format = '0.0000'\n",
    "                        else:  # Other metrics\n",
    "                            cell.number_format = '0.000000'\n",
    "        \n",
    "        # Conditional formatting untuk coverage\n",
    "        ws4.conditional_formatting.add(f\"E2:E{ws4.max_row}\", \n",
    "                                      ColorScaleRule(start_type='min', start_color='FFB6C1',\n",
    "                                                   end_type='max', end_color='90EE90'))\n",
    "        \n",
    "        ws4.freeze_panes = \"A2\"\n",
    "        ws4.auto_filter.ref = ws4.dimensions\n",
    "        ws4.row_dimensions[1].height = 30\n",
    "        \n",
    "        # Sheet 5: Model Statistics dan Comparison\n",
    "        ws5 = wb_cbow.create_sheet(\"Statistik_Model\")\n",
    "        \n",
    "        # Buat comprehensive model statistics\n",
    "        model_statistics = [\n",
    "            ['Metrik Model', 'Nilai', 'Deskripsi', 'Interpretasi'],\n",
    "            ['Parameter Arsitektur', '', '', ''],\n",
    "            ['Vector Dimensions', cbow_model.wv.vector_size, 'Ukuran embedding untuk setiap kata', 'Lebih tinggi = representasi lebih kaya'],\n",
    "            ['Vocabulary Size', len(cbow_model.wv), 'Jumlah kata unik yang dipelajari', 'Ukuran kosakata model'],\n",
    "            ['Window Size', cbow_model.window, 'Jumlah kata konteks kiri-kanan', 'Context window untuk prediksi'],\n",
    "            ['Min Count Threshold', cbow_model.min_count, 'Frekuensi minimum kata', 'Filter noise dan kata jarang'],\n",
    "            ['Training Epochs', cbow_model.epochs, 'Jumlah iterasi pelatihan', 'Tingkat pembelajaran model'],\n",
    "            ['', '', '', ''],\n",
    "            ['Statistik Dataset', '', '', ''],\n",
    "            ['Total Documents', len(sentences), 'Jumlah dokumen dalam corpus', 'Ukuran dataset training'],\n",
    "            ['Average Doc Length', f\"{np.mean([len(sent) for sent in sentences]):.1f}\", 'Rata-rata panjang dokumen', 'Kata per dokumen setelah preprocessing'],\n",
    "            ['Total Unique Words', len(set([word for sent in sentences for word in sent])), 'Kata unik dalam corpus', 'Kekayaan vocabulary corpus'],\n",
    "            ['Average Valid Words/Doc', f\"{np.mean(valid_words_count):.1f}\", 'Kata valid rata-rata per dokumen', 'Kata yang dikenali model'],\n",
    "            ['Average Coverage', f\"{np.mean([info['coverage'] for info in doc_info]):.4f}\", 'Persentase kata yang dikenali', 'Tingkat coverage model terhadap corpus'],\n",
    "            ['', '', '', ''], \n",
    "            ['Analisis Similarity', '', '', ''],\n",
    "            ['Average Similarity', f\"{np.mean(doc_similarity_matrix[np.triu_indices_from(doc_similarity_matrix, k=1)]):.6f}\", 'Rata-rata similarity antar dokumen', 'Tingkat kesamaan semantic corpus'],\n",
    "            ['Max Similarity', f\"{np.max(doc_similarity_matrix[np.triu_indices_from(doc_similarity_matrix, k=1)]):.6f}\", 'Similarity tertinggi', 'Dokumen paling mirip'],\n",
    "            ['Min Similarity', f\"{np.min(doc_similarity_matrix[np.triu_indices_from(doc_similarity_matrix, k=1)]):.6f}\", 'Similarity terendah', 'Dokumen paling berbeda'],\n",
    "            ['Std Similarity', f\"{np.std(doc_similarity_matrix[np.triu_indices_from(doc_similarity_matrix, k=1)]):.6f}\", 'Standar deviasi similarity', 'Variasi kesamaan antar dokumen'],\n",
    "            ['', '', '', ''],\n",
    "            ['Perbandingan dengan TF-IDF', '', '', ''],\n",
    "            ['Vector Type', 'Dense (padat)', 'TF-IDF: Sparse (jarang)', 'CBOW lebih compact'],\n",
    "            ['Dimensionality', '100 dimensi', 'TF-IDF: ~6475 dimensi', 'CBOW lebih efisien storage'],\n",
    "            ['Semantic Capture', 'Ya (context-based)', 'TF-IDF: Tidak (frequency-based)', 'CBOW menangkap makna'],\n",
    "            ['Interpretability', 'Similarity semantic', 'TF-IDF: Term importance', 'Berbeda tujuan analisis'],\n",
    "            ['Computational Cost', 'Training: Tinggi, Inference: Rendah', 'TF-IDF: Training: Rendah, Inference: Sedang', 'Trade-off waktu vs kualitas']\n",
    "        ]\n",
    "        \n",
    "        for row in model_statistics:\n",
    "            ws5.append(row)\n",
    "        \n",
    "        # Format sheet statistics\n",
    "        for row_idx, row in enumerate(ws5.iter_rows(min_row=1, max_row=len(model_statistics)), 1):\n",
    "            for col_idx, cell in enumerate(row):\n",
    "                if row_idx in [1, 2, 9, 16, 21]:  # Main headers\n",
    "                    cell.font = Font(bold=True, size=11, color=\"FFFFFF\")\n",
    "                    cell.fill = PatternFill(start_color=\"2E8B57\", end_color=\"2E8B57\", fill_type=\"solid\")\n",
    "                elif row_idx in [8, 15, 20]:  # Empty rows\n",
    "                    continue\n",
    "                else:\n",
    "                    cell.font = Font(size=10)\n",
    "                \n",
    "                cell.alignment = Alignment(horizontal=\"left\", vertical=\"top\", wrap_text=True)\n",
    "                cell.border = thin_border\n",
    "        \n",
    "        # Set column widths untuk statistics sheet\n",
    "        ws5.column_dimensions['A'].width = 25\n",
    "        ws5.column_dimensions['B'].width = 20\n",
    "        ws5.column_dimensions['C'].width = 35\n",
    "        ws5.column_dimensions['D'].width = 40\n",
    "        \n",
    "        # Simpan Excel file untuk CBOW\n",
    "        wb_cbow.save('CBOW_Analysis_Formatted.xlsx')\n",
    "        \n",
    "        print(\"✅ Excel CBOW berhasil dibuat dengan 5 sheet komprehensif:\")\n",
    "        print(\"   📊 CBOW_Analysis_Formatted.xlsx - Analisis lengkap CBOW\")\n",
    "        print(\"      • Sheet 1: Penjelasan_CBOW - Konsep dan teori CBOW vs TF-IDF\")\n",
    "        print(\"      • Sheet 2: Detail_Training - Sample word vectors dan similarity\")\n",
    "        print(\"      • Sheet 3: Analisis_Similarity - Top pasangan dokumen mirip\")\n",
    "        print(\"      • Sheet 4: Ringkasan_Embeddings - Summary document embeddings\")\n",
    "        print(\"      • Sheet 5: Statistik_Model - Comprehensive model statistics\")\n",
    "        print(f\"\\n📈 CBOW Analysis Summary:\")\n",
    "        print(f\"   • Total documents analyzed: {len(doc_info):,}\")\n",
    "        print(f\"   • Vocabulary size: {len(cbow_model.wv):,}\")\n",
    "        print(f\"   • Vector dimensions: {cbow_model.wv.vector_size}\")\n",
    "        print(f\"   • Average document similarity: {np.mean(doc_similarity_matrix[np.triu_indices_from(doc_similarity_matrix, k=1)]):.6f}\")\n",
    "        print(f\"   • Top document similarity: {np.max(doc_similarity_matrix[np.triu_indices_from(doc_similarity_matrix, k=1)]):.6f}\")\n",
    "        \n",
    "        cbow_excel_created = True\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"⚠️ openpyxl not available - creating CSV files instead:\")\n",
    "        \n",
    "        # Fallback ke CSV jika openpyxl tidak tersedia\n",
    "        doc_df = pd.DataFrame(doc_info)\n",
    "        doc_df['embedding_vector'] = [list(emb) for emb in document_embeddings]\n",
    "        doc_df.to_csv('CBOW_Document_Info.csv', index=False, encoding='utf-8-sig')\n",
    "        \n",
    "        similarity_df = pd.DataFrame(\n",
    "            doc_similarity_matrix,\n",
    "            columns=[f'Doc_{i}' for i in range(len(doc_similarity_matrix))],\n",
    "            index=[f'Doc_{i}' for i in range(len(doc_similarity_matrix))]\n",
    "        )\n",
    "        similarity_df.to_csv('CBOW_Document_Similarity_Matrix.csv', encoding='utf-8-sig')\n",
    "        \n",
    "        print(\"   📄 CBOW_Document_Info.csv - Document information with embeddings\")\n",
    "        print(\"   📄 CBOW_Document_Similarity_Matrix.csv - Similarity matrix\")\n",
    "        cbow_excel_created = False\n",
    "\n",
    "else:\n",
    "    print(\"⚠️ SKIP EXPORT CBOW - Data tidak tersedia\")\n",
    "    print(\"Export memerlukan model CBOW dan embeddings yang sudah berhasil dibuat.\")\n",
    "    print(\"Silakan perbaiki masalah gensim terlebih dahulu.\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **6. Perbandingan Hasil: TF-IDF vs CBOW**\n",
    "\n",
    "Setelah menjalankan analisis **TF-IDF** dan **CBOW**, kita dapat membandingkan kedua pendekatan ini:\n",
    "\n",
    "#### **🔍 Perbedaan Pendekatan:**\n",
    "\n",
    "| **Aspek** | **TF-IDF** | **CBOW** |\n",
    "|-----------|------------|----------|\n",
    "| **Jenis Vector** | Sparse (jarang) | Dense (padat) |\n",
    "| **Dimensi** | Sebesar vocabulary (6,475) | Fixed (100 dimensi) |\n",
    "| **Basis Perhitungan** | Frekuensi statistik | Semantic context |\n",
    "| **Interpretasi** | Importance berdasarkan frekuensi | Similarity berdasarkan konteks |\n",
    "| **Ukuran File** | Besar untuk sparse matrix | Lebih kecil untuk dense vectors |\n",
    "\n",
    "#### **📊 Output yang Dihasilkan:**\n",
    "\n",
    "**TF-IDF Files:**\n",
    "- `TF_IDF_Analysis_Formatted.xlsx` - Analisis lengkap dalam Excel\n",
    "- Matrix TF-IDF untuk klasifikasi dan pencarian\n",
    "\n",
    "**CBOW Files:**\n",
    "- `CBOW_Document_Info.csv` - Informasi dokumen dengan embeddings\n",
    "- `CBOW_Document_Similarity_Matrix.csv` - Matrix similarity semantik  \n",
    "- `CBOW_Top_Similar_Pairs.csv` - Pasangan dokumen paling mirip\n",
    "- `CBOW_Word_Embeddings.csv` - Word vectors untuk vocabulary\n",
    "- `CBOW_Model_Statistics.csv` - Statistik model CBOW\n",
    "\n",
    "#### **💡 Aplikasi Praktis:**\n",
    "\n",
    "- **TF-IDF**: Cocok untuk information retrieval, klasifikasi teks, keyword extraction\n",
    "- **CBOW**: Cocok untuk analisis semantic similarity, clustering berdasarkan makna, recommendation system\n",
    "\n",
    "Kedua pendekatan ini melengkapi satu sama lain dan dapat digunakan bersamaan tergantung pada tujuan analisis yang diinginkan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **6. Menghitung dan Menyimpan File Hasil TF-IDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Excel file created with comprehensive TF-IDF analysis:\n",
      "   📊 TF_IDF_Analysis_Formatted.xlsx - Analisis lengkap dengan 5 sheet terurut\n",
      "      • Sheet 1: Penjelasan_Rumus - Panduan lengkap rumus dan interpretasi (Bahasa Indonesia)\n",
      "      • Sheet 2: Perhitungan_TF - Detail perhitungan Term Frequency\n",
      "      • Sheet 3: Perhitungan_IDF - Detail perhitungan Inverse Document Frequency\n",
      "      • Sheet 4: Tabel_TF_IDF - Hasil akhir dengan conditional formatting\n",
      "      • Sheet 5: Statistik_Term - Ringkasan statistik dengan peringkat\n",
      "\n",
      "📈 Analysis Summary:\n",
      "   • Total term-document pairs: 40,184\n",
      "   • Unique terms: 4,850\n",
      "   • Documents analyzed: 649\n",
      "   • Top TF-IDF score: 0.823313\n",
      "   • Average TF-IDF score: 0.084527\n",
      "\n",
      "💡 Excel file contains all data with professional formatting - CSV files not needed!\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF Analysis - Complete Implementation with Indonesian Labels\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "# Siapkan data TF-IDF\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "tfidf_matrix = x.toarray()\n",
    "judul_jurnal = result_df['judul'].tolist()\n",
    "idf_scores = vectorizer.idf_\n",
    "\n",
    "# Buat TF-IDF table dengan kolom yang diminta\n",
    "tfidf_results = []\n",
    "for doc_idx in range(len(judul_jurnal)):\n",
    "    jurnal_name = judul_jurnal[doc_idx][:80] + \"...\" if len(judul_jurnal[doc_idx]) > 80 else judul_jurnal[doc_idx]\n",
    "    doc_tfidf = tfidf_matrix[doc_idx]\n",
    "    \n",
    "    for term_idx, tfidf_score in enumerate(doc_tfidf):\n",
    "        if tfidf_score > 0:  # Hanya term yang muncul di dokumen\n",
    "            term = feature_names[term_idx]\n",
    "            idf_score = idf_scores[term_idx]\n",
    "            \n",
    "            tfidf_results.append({\n",
    "                'Term': term,\n",
    "                'Nama_Jurnal': jurnal_name,\n",
    "                'IDF': round(idf_score, 4),\n",
    "                'TF_IDF': round(tfidf_score, 6)\n",
    "            })\n",
    "\n",
    "# Buat DataFrame TF-IDF\n",
    "tfidf_df = pd.DataFrame(tfidf_results)\n",
    "tfidf_df = tfidf_df.sort_values('TF_IDF', ascending=False)\n",
    "\n",
    "# Tambahkan nomor urut untuk kemudahan baca\n",
    "tfidf_df['No'] = range(1, len(tfidf_df) + 1)\n",
    "tfidf_df = tfidf_df[['No', 'Term', 'Nama_Jurnal', 'IDF', 'TF_IDF']]\n",
    "\n",
    "# Buat ringkasan per term\n",
    "term_stats = tfidf_df.groupby('Term').agg({\n",
    "    'IDF': 'first',\n",
    "    'TF_IDF': ['count', 'mean', 'max']\n",
    "}).round(6)\n",
    "term_stats.columns = ['IDF', 'Frekuensi_Muncul', 'Rata_TF_IDF', 'Max_TF_IDF']\n",
    "term_stats = term_stats.reset_index().sort_values('Rata_TF_IDF', ascending=False)\n",
    "term_stats['Peringkat'] = range(1, len(term_stats) + 1)\n",
    "term_stats = term_stats[['Peringkat', 'Term', 'IDF', 'Frekuensi_Muncul', 'Rata_TF_IDF', 'Max_TF_IDF']]\n",
    "\n",
    "# Coba buat file Excel dengan formatting profesional\n",
    "try:\n",
    "    from openpyxl import Workbook\n",
    "    from openpyxl.styles import Font, PatternFill, Border, Side, Alignment\n",
    "    from openpyxl.utils.dataframe import dataframe_to_rows\n",
    "    from openpyxl.formatting.rule import ColorScaleRule\n",
    "    \n",
    "    # Buat workbook baru\n",
    "    wb = Workbook()\n",
    "    \n",
    "    # Definisi styles\n",
    "    header_font = Font(bold=True, color=\"FFFFFF\", size=11)\n",
    "    header_fill = PatternFill(start_color=\"366092\", end_color=\"366092\", fill_type=\"solid\")\n",
    "    header_alignment = Alignment(horizontal=\"center\", vertical=\"center\", wrap_text=True)\n",
    "    \n",
    "    data_font = Font(size=9)\n",
    "    data_alignment_left = Alignment(horizontal=\"left\", vertical=\"center\")\n",
    "    data_alignment_center = Alignment(horizontal=\"center\", vertical=\"center\")\n",
    "    data_alignment_right = Alignment(horizontal=\"right\", vertical=\"center\")\n",
    "    \n",
    "    thin_border = Border(\n",
    "        left=Side(style='thin'), right=Side(style='thin'),\n",
    "        top=Side(style='thin'), bottom=Side(style='thin')\n",
    "    )\n",
    "    \n",
    "    # Sheet 1: Penjelasan Rumus (Formula Explanation)\n",
    "    ws1 = wb.active\n",
    "    ws1.title = \"Penjelasan_Rumus\"\n",
    "    \n",
    "    # Penjelasan rumus dalam bahasa Indonesia\n",
    "    penjelasan_rumus = [\n",
    "        ['Metrik', 'Rumus', 'Penjelasan', 'Contoh'],\n",
    "        ['TF (Term Frequency)', 'TF = jumlah_term / total_kata_dalam_dokumen', 'Frekuensi kemunculan term dalam dokumen relatif terhadap panjang dokumen', 'Jika \"analisis\" muncul 3 kali dalam dokumen 100 kata: TF = 3/100 = 0.03'],\n",
    "        ['IDF (Inverse Document Frequency)', 'IDF = log(total_dokumen / dokumen_yang_mengandung_term)', 'Tingkat kepentingan term di seluruh koleksi dokumen', 'Jika \"analisis\" muncul di 50 dari 1000 dokumen: IDF = log(1000/50) = 2.996'],\n",
    "        ['TF-IDF', 'TF-IDF = TF × IDF', 'Gabungan skor kepentingan dan frekuensi', 'TF-IDF = 0.03 × 2.996 = 0.08988'],\n",
    "        ['', '', '', ''],\n",
    "        ['Panduan Interpretasi', '', '', ''],\n",
    "        ['TF-IDF Tinggi (>0.1)', 'Term sering muncul di dokumen DAN jarang di koleksi', 'Term yang sangat karakteristik untuk dokumen', 'Istilah teknis spesifik penelitian'],\n",
    "        ['TF-IDF Sedang (0.01-0.1)', 'Term yang cukup penting', 'Relevan tapi tidak unik untuk dokumen', 'Istilah penelitian umum'],\n",
    "        ['TF-IDF Rendah (<0.01)', 'Term umum atau jarang muncul', 'Kurang karakteristik untuk dokumen', 'Kata umum, stopwords'],\n",
    "        ['', '', '', ''],\n",
    "        ['Statistik Data Penelitian', '', '', ''],\n",
    "        [f'Total Dokumen Dianalisis', str(len(judul_jurnal)), 'Jumlah abstrak penelitian yang diproses', ''],\n",
    "        [f'Term Unik Ditemukan', str(len(feature_names)), 'Ukuran kosakata setelah preprocessing', ''],\n",
    "        [f'Total Pasangan Term-Dokumen', str(len(tfidf_df)), 'Nilai TF-IDF yang tidak nol', ''],\n",
    "        [f'Rata-rata Skor TF-IDF', f\"{tfidf_df['TF_IDF'].mean():.6f}\", 'Rata-rata kepentingan semua term', ''],\n",
    "        [f'Skor TF-IDF Tertinggi', f\"{tfidf_df['TF_IDF'].max():.6f}\", 'Pasangan term-dokumen paling karakteristik', ''],\n",
    "        ['', '', '', ''],\n",
    "        ['Tahapan Preprocessing Teks', '', '', ''],\n",
    "        ['1. Pembersihan Teks', 'Lowercasing, hapus angka & tanda baca', 'Normalisasi teks untuk konsistensi', 'ANALISIS -> analisis'],\n",
    "        ['2. Tokenisasi', 'Pemecahan kalimat menjadi kata', 'Memisahkan teks menjadi unit kata', '\"data mining\" -> [\"data\", \"mining\"]'],\n",
    "        ['3. Stopwords Removal', 'Hapus kata umum (dan, atau, di, dll)', 'Fokus pada kata bermakna', '[\"penelitian\", \"dan\", \"data\"] -> [\"penelitian\", \"data\"]'],\n",
    "        ['4. Stemming', 'Ubah kata ke bentuk dasar', 'Mengurangi variasi bentuk kata', 'penelitian -> teliti, menganalisis -> analisis']\n",
    "    ]\n",
    "    \n",
    "    for row in penjelasan_rumus:\n",
    "        ws1.append(row)\n",
    "    \n",
    "    # Format sheet penjelasan\n",
    "    for row_idx, row in enumerate(ws1.iter_rows(min_row=1, max_row=len(penjelasan_rumus)), 1):\n",
    "        for col_idx, cell in enumerate(row):\n",
    "            if row_idx == 1 or row_idx == 6 or row_idx == 11 or row_idx == 17:  # Headers\n",
    "                cell.font = Font(bold=True, size=11, color=\"FFFFFF\")\n",
    "                cell.fill = PatternFill(start_color=\"366092\", end_color=\"366092\", fill_type=\"solid\")\n",
    "            elif row_idx in [5, 10, 16]:  # Empty rows\n",
    "                continue\n",
    "            else:\n",
    "                cell.font = Font(size=10)\n",
    "                if col_idx == 1:  # Formula column\n",
    "                    cell.font = Font(size=9, name=\"Courier New\")\n",
    "            \n",
    "            cell.alignment = Alignment(horizontal=\"left\", vertical=\"top\", wrap_text=True)\n",
    "            cell.border = thin_border\n",
    "    \n",
    "    # Set column widths untuk penjelasan sheet\n",
    "    ws1.column_dimensions['A'].width = 30\n",
    "    ws1.column_dimensions['B'].width = 45\n",
    "    ws1.column_dimensions['C'].width = 40\n",
    "    ws1.column_dimensions['D'].width = 45\n",
    "    \n",
    "    # Sheet 2: Perhitungan TF (TF Calculation Details)\n",
    "    ws2 = wb.create_sheet(\"Perhitungan_TF\")\n",
    "    \n",
    "    # Buat data TF calculation untuk beberapa dokumen pertama\n",
    "    tf_details = []\n",
    "    for doc_idx in range(min(15, len(judul_jurnal))):  # Ambil 15 dokumen pertama\n",
    "        jurnal_name = judul_jurnal[doc_idx][:60] + \"...\" if len(judul_jurnal[doc_idx]) > 60 else judul_jurnal[doc_idx]\n",
    "        doc_tfidf = tfidf_matrix[doc_idx]\n",
    "        \n",
    "        # Hitung total words dalam dokumen\n",
    "        doc_words = stemmed_texts[doc_idx].split()\n",
    "        total_words = len(doc_words)\n",
    "        word_counts = {}\n",
    "        for word in doc_words:\n",
    "            word_counts[word] = word_counts.get(word, 0) + 1\n",
    "        \n",
    "        # Ambil top 8 terms untuk dokumen ini\n",
    "        doc_terms = [(i, score) for i, score in enumerate(doc_tfidf) if score > 0]\n",
    "        doc_terms.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        for term_idx, tfidf_score in doc_terms[:8]:  # Top 8 terms\n",
    "            term = feature_names[term_idx]\n",
    "            term_frequency = word_counts.get(term, 0)\n",
    "            tf_score = term_frequency / total_words if total_words > 0 else 0\n",
    "            \n",
    "            tf_details.append({\n",
    "                'ID_Dokumen': doc_idx + 1,\n",
    "                'Nama_Jurnal': jurnal_name,\n",
    "                'Term': term,\n",
    "                'Jumlah_Term': term_frequency,\n",
    "                'Total_Kata': total_words,\n",
    "                'Skor_TF': round(tf_score, 6),\n",
    "                'Rumus': f\"{term_frequency}/{total_words}\"\n",
    "            })\n",
    "    \n",
    "    tf_df = pd.DataFrame(tf_details)\n",
    "    \n",
    "    # Tulis TF data\n",
    "    for r in dataframe_to_rows(tf_df, index=False, header=True):\n",
    "        ws2.append(r)\n",
    "    \n",
    "    # Format header sheet 2\n",
    "    for cell in ws2[1]:\n",
    "        cell.font = header_font\n",
    "        cell.fill = header_fill\n",
    "        cell.alignment = header_alignment\n",
    "        cell.border = thin_border\n",
    "    \n",
    "    # Set column widths untuk TF sheet\n",
    "    ws2_widths = [12, 45, 20, 12, 12, 15, 15]\n",
    "    ws2_alignments = [data_alignment_center, data_alignment_left, data_alignment_left,\n",
    "                     data_alignment_center, data_alignment_center, data_alignment_right, data_alignment_center]\n",
    "    \n",
    "    for idx, (width, alignment) in enumerate(zip(ws2_widths, ws2_alignments)):\n",
    "        col_letter = chr(65 + idx)\n",
    "        ws2.column_dimensions[col_letter].width = width\n",
    "        \n",
    "        for row in ws2.iter_rows(min_row=2, min_col=idx+1, max_col=idx+1):\n",
    "            for cell in row:\n",
    "                cell.font = data_font\n",
    "                cell.alignment = alignment\n",
    "                cell.border = thin_border\n",
    "                if idx in [3, 4]:  # Count columns\n",
    "                    cell.number_format = '0'\n",
    "                elif idx == 5:  # TF_Score\n",
    "                    cell.number_format = '0.000000'\n",
    "    \n",
    "    # Conditional formatting untuk TF Score\n",
    "    ws2.conditional_formatting.add(f\"F2:F{ws2.max_row}\", \n",
    "                                  ColorScaleRule(start_type='min', start_color='FFFFFF',\n",
    "                                               end_type='max', end_color='FFE699'))\n",
    "    \n",
    "    ws2.freeze_panes = \"A2\"\n",
    "    ws2.auto_filter.ref = ws2.dimensions\n",
    "    ws2.row_dimensions[1].height = 30\n",
    "    \n",
    "    # Sheet 3: Perhitungan IDF (IDF Calculation Details)\n",
    "    ws3 = wb.create_sheet(\"Perhitungan_IDF\")\n",
    "    \n",
    "    # Buat data IDF calculation\n",
    "    idf_details = []\n",
    "    total_documents = len(judul_jurnal)\n",
    "    \n",
    "    # Ambil top 50 terms berdasarkan rata-rata TF-IDF\n",
    "    top_terms = term_stats.head(50)\n",
    "    \n",
    "    for _, row in top_terms.iterrows():\n",
    "        term = row['Term']\n",
    "        term_idx = list(feature_names).index(term)\n",
    "        idf_score = idf_scores[term_idx]\n",
    "        \n",
    "        # Hitung berapa dokumen yang mengandung term ini\n",
    "        docs_containing_term = sum(1 for doc_tfidf in tfidf_matrix if doc_tfidf[term_idx] > 0)\n",
    "        calculated_idf = math.log(total_documents / docs_containing_term) if docs_containing_term > 0 else 0\n",
    "        \n",
    "        idf_details.append({\n",
    "            'Term': term,\n",
    "            'Total_Dokumen': total_documents,\n",
    "            'Dokumen_Mengandung_Term': docs_containing_term,\n",
    "            'Skor_IDF': round(idf_score, 6),\n",
    "            'IDF_Terhitung': round(calculated_idf, 6),\n",
    "            'Rumus': f\"log({total_documents}/{docs_containing_term})\",\n",
    "            'Persentase_Frekuensi': round((docs_containing_term/total_documents)*100, 2)\n",
    "        })\n",
    "    \n",
    "    idf_df = pd.DataFrame(idf_details)\n",
    "    \n",
    "    # Tulis IDF data\n",
    "    for r in dataframe_to_rows(idf_df, index=False, header=True):\n",
    "        ws3.append(r)\n",
    "    \n",
    "    # Format header sheet 3\n",
    "    for cell in ws3[1]:\n",
    "        cell.font = header_font\n",
    "        cell.fill = header_fill\n",
    "        cell.alignment = header_alignment\n",
    "        cell.border = thin_border\n",
    "    \n",
    "    # Set column widths untuk IDF sheet\n",
    "    ws3_widths = [20, 15, 20, 15, 15, 18, 15]\n",
    "    ws3_alignments = [data_alignment_left, data_alignment_center, data_alignment_center,\n",
    "                     data_alignment_right, data_alignment_right, data_alignment_center, data_alignment_right]\n",
    "    \n",
    "    for idx, (width, alignment) in enumerate(zip(ws3_widths, ws3_alignments)):\n",
    "        col_letter = chr(65 + idx)\n",
    "        ws3.column_dimensions[col_letter].width = width\n",
    "        \n",
    "        for row in ws3.iter_rows(min_row=2, min_col=idx+1, max_col=idx+1):\n",
    "            for cell in row:\n",
    "                cell.font = data_font\n",
    "                cell.alignment = alignment\n",
    "                cell.border = thin_border\n",
    "                if idx in [1, 2]:  # Document counts\n",
    "                    cell.number_format = '0'\n",
    "                elif idx in [3, 4, 6]:  # IDF scores and percentage\n",
    "                    if idx == 6:  # Percentage\n",
    "                        cell.number_format = '0.00%'\n",
    "                    else:  # IDF scores\n",
    "                        cell.number_format = '0.000000'\n",
    "    \n",
    "    # Conditional formatting untuk frequency percentage\n",
    "    ws3.conditional_formatting.add(f\"G2:G{ws3.max_row}\", \n",
    "                                  ColorScaleRule(start_type='max', start_color='FF9999',\n",
    "                                               end_type='min', end_color='99FF99'))\n",
    "    \n",
    "    ws3.freeze_panes = \"A2\"\n",
    "    ws3.auto_filter.ref = ws3.dimensions\n",
    "    ws3.row_dimensions[1].height = 30\n",
    "    \n",
    "    # Sheet 4: Tabel TF-IDF (Final Results)\n",
    "    ws4 = wb.create_sheet(\"Tabel_TF_IDF\")\n",
    "    \n",
    "    # Tulis data TF-IDF hasil akhir\n",
    "    for r in dataframe_to_rows(tfidf_df, index=False, header=True):\n",
    "        ws4.append(r)\n",
    "    \n",
    "    # Format header sheet 4\n",
    "    for cell in ws4[1]:\n",
    "        cell.font = header_font\n",
    "        cell.fill = header_fill\n",
    "        cell.alignment = header_alignment\n",
    "        cell.border = thin_border\n",
    "    \n",
    "    # Set column widths dan format data\n",
    "    ws4_widths = [8, 25, 70, 12, 15]\n",
    "    ws4_alignments = [data_alignment_center, data_alignment_left, data_alignment_left, \n",
    "                     data_alignment_right, data_alignment_right]\n",
    "    \n",
    "    for idx, (width, alignment) in enumerate(zip(ws4_widths, ws4_alignments)):\n",
    "        col_letter = chr(65 + idx)\n",
    "        ws4.column_dimensions[col_letter].width = width\n",
    "        \n",
    "        # Format data cells\n",
    "        for row in ws4.iter_rows(min_row=2, min_col=idx+1, max_col=idx+1):\n",
    "            for cell in row:\n",
    "                cell.font = data_font\n",
    "                cell.alignment = alignment\n",
    "                cell.border = thin_border\n",
    "                if idx in [3, 4]:  # IDF dan TF_IDF columns\n",
    "                    cell.number_format = '0.000000'\n",
    "    \n",
    "    # Conditional formatting untuk TF_IDF\n",
    "    ws4.conditional_formatting.add(f\"E2:E{ws4.max_row}\", \n",
    "                                  ColorScaleRule(start_type='min', start_color='FFFFFF',\n",
    "                                               mid_type='percentile', mid_value=50, mid_color='FFE699',\n",
    "                                               end_type='max', end_color='FF9999'))\n",
    "    \n",
    "    # Freeze panes dan filter\n",
    "    ws4.freeze_panes = \"A2\"\n",
    "    ws4.auto_filter.ref = ws4.dimensions\n",
    "    ws4.row_dimensions[1].height = 30\n",
    "    \n",
    "    # Sheet 5: Statistik Term (Term Statistics)\n",
    "    ws5 = wb.create_sheet(\"Statistik_Term\")\n",
    "    \n",
    "    # Tulis data ringkasan term statistics\n",
    "    for r in dataframe_to_rows(term_stats, index=False, header=True):\n",
    "        ws5.append(r)\n",
    "    \n",
    "    # Format header sheet 5\n",
    "    for cell in ws5[1]:\n",
    "        cell.font = header_font\n",
    "        cell.fill = header_fill\n",
    "        cell.alignment = header_alignment\n",
    "        cell.border = thin_border\n",
    "    \n",
    "    # Set column widths dan format untuk sheet 5\n",
    "    ws5_widths = [10, 25, 12, 15, 15, 15]\n",
    "    ws5_alignments = [data_alignment_center, data_alignment_left, data_alignment_right,\n",
    "                     data_alignment_center, data_alignment_right, data_alignment_right]\n",
    "    \n",
    "    for idx, (width, alignment) in enumerate(zip(ws5_widths, ws5_alignments)):\n",
    "        col_letter = chr(65 + idx)\n",
    "        ws5.column_dimensions[col_letter].width = width\n",
    "        \n",
    "        # Format data cells\n",
    "        for row in ws5.iter_rows(min_row=2, min_col=idx+1, max_col=idx+1):\n",
    "            for cell in row:\n",
    "                cell.font = data_font\n",
    "                cell.alignment = alignment\n",
    "                cell.border = thin_border\n",
    "                if idx in [2, 4, 5]:  # Numerical columns\n",
    "                    cell.number_format = '0.000000'\n",
    "                elif idx == 3:  # Frekuensi column\n",
    "                    cell.number_format = '0'\n",
    "    \n",
    "    # Conditional formatting untuk rata TF_IDF\n",
    "    ws5.conditional_formatting.add(f\"E2:E{ws5.max_row}\", \n",
    "                                  ColorScaleRule(start_type='min', start_color='E8F5E8',\n",
    "                                               end_type='max', end_color='4CAF50'))\n",
    "    \n",
    "    # Freeze panes dan filter untuk sheet 5\n",
    "    ws5.freeze_panes = \"A2\"\n",
    "    ws5.auto_filter.ref = ws5.dimensions\n",
    "    ws5.row_dimensions[1].height = 30\n",
    "    \n",
    "    # Simpan Excel\n",
    "    wb.save('TF_IDF_Analysis_Formatted.xlsx')\n",
    "    \n",
    "    print(\"✅ Excel file created with comprehensive TF-IDF analysis:\")\n",
    "    print(\"   📊 TF_IDF_Analysis_Formatted.xlsx - Analisis lengkap dengan 5 sheet terurut\")\n",
    "    print(\"      • Sheet 1: Penjelasan_Rumus - Panduan lengkap rumus dan interpretasi (Bahasa Indonesia)\")\n",
    "    print(\"      • Sheet 2: Perhitungan_TF - Detail perhitungan Term Frequency\")\n",
    "    print(\"      • Sheet 3: Perhitungan_IDF - Detail perhitungan Inverse Document Frequency\")\n",
    "    print(\"      • Sheet 4: Tabel_TF_IDF - Hasil akhir dengan conditional formatting\")\n",
    "    print(\"      • Sheet 5: Statistik_Term - Ringkasan statistik dengan peringkat\")\n",
    "    excel_created = True\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"⚠️ openpyxl not available - creating CSV files instead:\")\n",
    "    \n",
    "    # Simpan ke CSV dengan formatting yang rapi\n",
    "    tfidf_df.to_csv('TF_IDF_Table.csv', index=False, encoding='utf-8-sig', float_format='%.6f')\n",
    "    term_stats.to_csv('Term_Statistics.csv', index=False, encoding='utf-8-sig', float_format='%.6f')\n",
    "    \n",
    "    print(\"   📄 TF_IDF_Table.csv - Main TF-IDF data with numbering\")\n",
    "    print(\"   📄 Term_Statistics.csv - Term summary with rankings\")\n",
    "    excel_created = False\n",
    "\n",
    "print(f\"\\n📈 Analysis Summary:\")\n",
    "print(f\"   • Total term-document pairs: {len(tfidf_df):,}\")\n",
    "print(f\"   • Unique terms: {tfidf_df['Term'].nunique():,}\")\n",
    "print(f\"   • Documents analyzed: {tfidf_df['Nama_Jurnal'].nunique():,}\")\n",
    "print(f\"   • Top TF-IDF score: {tfidf_df['TF_IDF'].max():.6f}\")\n",
    "print(f\"   • Average TF-IDF score: {tfidf_df['TF_IDF'].mean():.6f}\")\n",
    "\n",
    "if 'excel_created' in locals() and excel_created:\n",
    "    print(f\"\\n💡 Excel file contains all data with professional formatting - CSV files not needed!\")\n",
    "else:\n",
    "    print(f\"\\n💡 Use CSV files for data analysis or install openpyxl for better Excel formatting.\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}