{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-10-13T05:19:27.700653Z",
     "iopub.status.busy": "2025-10-13T05:19:27.700653Z",
     "iopub.status.idle": "2025-10-13T05:19:35.252291Z",
     "shell.execute_reply": "2025-10-13T05:19:35.252291Z"
    },
    "id": "33OZHzkLJrts",
    "outputId": "37adac86-13d8-41ea-f4a7-9c0e69fe5944"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import string\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from gensim import corpora, models\n",
    "from gensim.models import CoherenceModel\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scraping berita kompas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T05:19:35.252291Z",
     "iopub.status.busy": "2025-10-13T05:19:35.252291Z",
     "iopub.status.idle": "2025-10-13T05:19:36.346702Z",
     "shell.execute_reply": "2025-10-13T05:19:36.346702Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping berhasil. Teks mentah siap diproses.\n"
     ]
    }
   ],
   "source": [
    "web = requests.get(\"https://surabaya.kompas.com/read/2025/10/13/073714878/keharuan-keluarga-saat-identitas-2-jasad-santri-ponpes-al-khoziny\")\n",
    "soup = BeautifulSoup(web.content, 'html.parser')\n",
    "for a in soup(['script', 'style']):\n",
    "    a.decompose()\n",
    "text = ' '.join(soup.stripped_strings)\n",
    "print(\"Scraping berhasil. Teks mentah siap diproses.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## inisisalisasi preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T05:19:36.424839Z",
     "iopub.status.busy": "2025-10-13T05:19:36.424839Z",
     "iopub.status.idle": "2025-10-13T05:19:36.640086Z",
     "shell.execute_reply": "2025-10-13T05:19:36.640086Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmer dan Stopwords siap digunakan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Membuat stemmer dari Sastrawi\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "\n",
    "# Mengunduh dan menyiapkan daftar stopwords Bahasa Indonesia\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('indonesian'))\n",
    "\n",
    "print(\"Stemmer dan Stopwords siap digunakan.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocessing per kalimat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-10-13T05:19:36.640086Z",
     "iopub.status.busy": "2025-10-13T05:19:36.640086Z",
     "iopub.status.idle": "2025-10-13T05:20:42.494784Z",
     "shell.execute_reply": "2025-10-13T05:20:42.494784Z"
    },
    "id": "QsC5GUtcWBEL",
    "outputId": "04981d7b-7221-41f7-9913-abc7f95d1248"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teks berhasil dipecah menjadi 56 kalimat.\n",
      "\n",
      "Preprocessing selesai. Dihasilkan 42 dokumen yang siap diproses.\n",
      "\n",
      "Contoh 3 dokumen pertama:\n",
      "Dokumen 0: ['haru', 'keluarga', 'identitas', 'jasad', 'santri', 'ponpes', 'al', 'khoziny', 'identifikasi', 'otomatis', 'mode', 'gelap', 'mode', 'terang', 'login', 'gabung', 'kompas']\n",
      "Dokumen 1: ['com', 'konten', 'simpan', 'konten', 'suka', 'atur', 'minat', 'ikan', 'masuk', 'langgan', 'kompas', 'one', 'news', 'nasional', 'global', 'megapolitan', 'regional', 'milu', 'hype', 'konsultasi', 'hukum', 'cek', 'fakta', 'surat', 'baca', 'indeks', 'kilas', 'daerah', 'kilas', 'korporasi', 'kilas', 'menteri', 'sorot', 'politik', 'kilas', 'badan', 'negara', 'kelana', 'indonesia', 'kalbe', 'health', 'corner', 'kilas', 'parlemen', 'kilas', 'bumn', 'nusaraya', 'sumatera', 'utara', 'sumatera', 'selatan', 'sumatera', 'barat', 'riau', 'lampung', 'banten', 'yogyakarta', 'jawa', 'barat', 'jawa', 'jawa', 'timur', 'kalimantan', 'barat', 'kalimantan', 'timur', 'sulawesi', 'selatan', 'bal', 'indeks', 'jagat', 'literasi', 'artikel', 'foto', 'video', 'rolls', 'donasi', 'stem', 'cahaya', 'aktual', 'doa', 'niat', 'jadwal', 'sholat', 'tekno', 'apps', 'os', 'gadget', 'internet', 'hardware', 'business', 'game', 'galeri', 'indeks', 'tech', 'innovation', 'kilas', 'internet', 'otomotif', 'motor', 'mobil', 'sport', 'niaga', 'komunitas', 'otopedia', 'rapah', 'ev', 'leadership', 'elektrifikasi', 'pamer', 'bola', 'timnas', 'indonesia', 'liga', 'indonesia', 'liga', 'inggris', 'liga', 'italia', 'liga', 'champions', 'klasemen', 'sports', 'motogp', 'badminton', 'indeks', 'lifestyle', 'wellness', 'fashion', 'relationship', 'parenting', 'beauty', 'grooming', 'buku', 'indeks', 'sadar', 'stunting', 'kilas', 'lifestyle', 'tren', 'lestari', 'health', 'sakit', 'az', 'kilas', 'sehat', 'money', 'ekbis', 'uang', 'syariah', 'industri', 'energi', 'karier', 'cuan', 'belanja', 'pajak', 'indeks', 'kilas', 'badan', 'kilas', 'transportasi', 'kilas', 'fintech', 'kilas', 'perban', 'kilas', 'investasi', 'transaksi', 'digital', 'jejak', 'umkm', 'properti', 'news', 'listing', 'properti', 'arsitektur', 'konstruksi', 'tips', 'properti', 'ikn', 'homey', 'indeks', 'sorot', 'properti', 'edukasi', 'sekolah', 'edu', 'news', 'guru', 'didik', 'khusus', 'beasiswa', 'literasi', 'skola', 'kilas', 'didik', 'ideaksi', 'travel', 'travel', 'news', 'travel', 'ideas', 'hotel', 'story', 'travelpedia', 'food', 'ohayo', 'jepang', 'indeks', 'video', 'parapuan', 'kolom', 'sains', 'jeo', 'foto', 'vik', 'netizen', 'warta', 'membership', 'kompas']\n",
      "Dokumen 2: ['com', 'perintah', 'swasta', 'lsmfigur', 'bumn', 'umkm', 'nusatirta', 'sehat', 'hidup', 'sehat', 'sejahtera', 'air', 'bersih', 'sanitasi', 'layak', 'didik', 'didik', 'kualitas', 'lingkung', 'energi', 'bersih', 'jangkau', 'tangan', 'ubah', 'iklim', 'ekosistem', 'laut', 'ekosistem', 'darat', 'ekonomi', 'umkm', 'miskin', 'lapar', 'tara', 'gender', 'kerja', 'layak', 'tumbuh', 'ekonomi', 'industri', 'inovasi', 'infrastruktur', 'senjang', 'kota', 'mukim', 'konsumsi', 'produksi', 'bertanggungjawab', 'program', 'lestari', 'lihat', 'haru', 'keluarga', 'identitas', 'jasad', 'santri', 'ponpes', 'al', 'khoziny', 'identifikasi', 'komentar', 'baca', 'berita', 'iklan']\n"
     ]
    }
   ],
   "source": [
    "# Memecah teks mentah menjadi daftar kalimat (sentences)\n",
    "sentences = re.split(r'[.!?]\\s*', text) # Memecah berdasarkan tanda titik, tanda tanya, atau tanda seru diikuti oleh spasi\n",
    "print(f\"Teks berhasil dipecah menjadi {len(sentences)} kalimat.\\n\") # Ditambah \\n\n",
    "\n",
    "#perulangan untuk preprocessing pada setiap kalimat \n",
    "processed_docs = []\n",
    "for sentence in sentences:\n",
    "    # Hanya proses kalimat yang tidak terlalu pendek\n",
    "    if len(sentence) > 20:\n",
    "        #Case Folding & Cleaning (angka, tanda baca)\n",
    "        sent = sentence.lower()\n",
    "        sent = re.sub(r'\\d+', '', sent)\n",
    "        sent = sent.translate(str.maketrans('', '', string.punctuation))\n",
    "        sent = sent.strip()\n",
    "\n",
    "        #Stemming\n",
    "        stemmed_sent = stemmer.stem(sent)\n",
    "\n",
    "        #Tokenizing & Stopword Removal\n",
    "        tokens = [word for word in stemmed_sent.split() if word not in stop_words]\n",
    "\n",
    "        # Tambahkan hasil (daftar token) jika tidak kosong\n",
    "        if tokens:\n",
    "            processed_docs.append(tokens)\n",
    "\n",
    "print(f\"Preprocessing selesai. Dihasilkan {len(processed_docs)} dokumen yang siap diproses.\")\n",
    "print(\"\\nContoh 3 dokumen pertama:\")\n",
    "for i, doc in enumerate(processed_docs[:3]):\n",
    "    print(f\"Dokumen {i}: {doc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-10-13T05:20:42.494784Z",
     "iopub.status.busy": "2025-10-13T05:20:42.494784Z",
     "iopub.status.idle": "2025-10-13T05:20:42.515523Z",
     "shell.execute_reply": "2025-10-13T05:20:42.515523Z"
    },
    "id": "2oriBqcCZ3zz",
    "outputId": "14dcc0ee-8b7c-4d27-9073-fa724d521ebf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Corpus berhasil dibuat.\n",
      "Contoh representasi BoW untuk dokumen pertama:\n",
      "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 2), (12, 1), (13, 1), (14, 1), (15, 1)]\n"
     ]
    }
   ],
   "source": [
    "# Membuat kamus: memetakan setiap kata unik ke sebuah ID\n",
    "dictionary = corpora.Dictionary(processed_docs)\n",
    "\n",
    "# Membuat corpus: mengubah setiap dokumen menjadi representasi BoW (ID kata, frekuensi)\n",
    "corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "print(\"\\nCorpus berhasil dibuat.\")\n",
    "print(\"Contoh representasi BoW untuk dokumen pertama:\")\n",
    "print(corpus[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T05:20:42.523005Z",
     "iopub.status.busy": "2025-10-13T05:20:42.515523Z",
     "iopub.status.idle": "2025-10-13T05:20:43.176323Z",
     "shell.execute_reply": "2025-10-13T05:20:43.176323Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memulai pelatihan model LDA untuk menemukan 5 topik...\n",
      "Model LDA berhasil dilatih!\n"
     ]
    }
   ],
   "source": [
    "# Tentukan jumlah topik yang ingin dicari\n",
    "num_topics = 5\n",
    "\n",
    "print(f\"Memulai pelatihan model LDA untuk menemukan {num_topics} topik...\")\n",
    "if not corpus or not dictionary:\n",
    "    print(\"Corpus atau Dictionary kosong. Tidak dapat melatih model LDA.\")\n",
    "else:\n",
    "    lda_model = models.LdaModel(\n",
    "        corpus=corpus,\n",
    "        id2word=dictionary,\n",
    "        num_topics=num_topics,\n",
    "        random_state=100,  # Agar hasil bisa direproduksi\n",
    "        passes=15          # Jumlah iterasi pelatihan\n",
    "    )\n",
    "    print(\"Model LDA berhasil dilatih!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T05:20:43.176323Z",
     "iopub.status.busy": "2025-10-13T05:20:43.176323Z",
     "iopub.status.idle": "2025-10-13T05:20:43.562063Z",
     "shell.execute_reply": "2025-10-13T05:20:43.562063Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topik-topik yang ditemukan oleh model LDA:\n",
      "--------------------------------------------------\n",
      "Topik: 0 \n",
      "Kata Kunci: 0.029*\"kilas\" + 0.016*\"indeks\" + 0.012*\"properti\" + 0.010*\"news\" + 0.009*\"tewas\" + 0.009*\"jenazah\" + 0.008*\"artikel\" + 0.008*\"indonesia\" + 0.008*\"travel\" + 0.008*\"nomor\"\n",
      "\n",
      "Topik: 1 \n",
      "Kata Kunci: 0.033*\"body\" + 0.028*\"part\" + 0.017*\"com\" + 0.013*\"identifikasi\" + 0.012*\"berita\" + 0.012*\"cocok\" + 0.012*\"app\" + 0.012*\"forensik\" + 0.012*\"tugas\" + 0.012*\"wahyu\"\n",
      "\n",
      "Topik: 2 \n",
      "Kata Kunci: 0.021*\"com\" + 0.013*\"kompas\" + 0.011*\"media\" + 0.008*\"gramedia\" + 0.008*\"layak\" + 0.008*\"iklim\" + 0.006*\"sekolah\" + 0.006*\"login\" + 0.006*\"iklan\" + 0.006*\"berita\"\n",
      "\n",
      "Topik: 3 \n",
      "Kata Kunci: 0.048*\"ponpes\" + 0.045*\"surabaya\" + 0.036*\"wib\" + 0.034*\"al\" + 0.032*\"khoziny\" + 0.030*\"korban\" + 0.022*\"identifikasi\" + 0.016*\"ambruk\" + 0.012*\"pamekasan\" + 0.012*\"santri\"\n",
      "\n",
      "Topik: 4 \n",
      "Kata Kunci: 0.032*\"jenazah\" + 0.029*\"kantong\" + 0.020*\"korban\" + 0.017*\"identifikasi\" + 0.017*\"dna\" + 0.017*\"proses\" + 0.015*\"rs\" + 0.015*\"bhayangkara\" + 0.013*\"khusnan\" + 0.013*\"polri\"\n",
      "\n",
      "--------------------------------------------------\n",
      "Coherence Score (c_v): 0.4390\n",
      "(Skor Coherence yang baik biasanya mendekati 1.0)\n"
     ]
    }
   ],
   "source": [
    "if 'lda_model' in locals():\n",
    "    print(\"Topik-topik yang ditemukan oleh model LDA:\")\n",
    "    print(\"-\" * 50)\n",
    "    for idx, topic in lda_model.print_topics(-1):\n",
    "        print(f\"Topik: {idx} \\nKata Kunci: {topic}\\n\")\n",
    "\n",
    "    # Evaluasi dengan Coherence Score dengan c_v\n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=processed_docs, dictionary=dictionary, coherence='c_v')\n",
    "    coherence_lda = coherence_model_lda.get_coherence()\n",
    "    print(\"-\" * 50)\n",
    "    print(f'Coherence Score (c_v): {coherence_lda:.4f}')\n",
    "    print(\"(Skor Coherence yang baik biasanya mendekati 1.0)\")\n",
    "else:\n",
    "    print(\"Model LDA belum dilatih. Jalankan sel sebelumnya terlebih dahulu.\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
